{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b47f7e-e48e-435a-a5a6-590cce68a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import dotenv\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import glob\n",
    "import hashlib\n",
    "import shutil\n",
    "\n",
    "config_train = \"config/UrbanElementsReID_train.yml\"  # \"config/UAM_containers.yml\"\n",
    "#config_test = \"config/UrbanElementsReID_test.yml\"\n",
    "config_test = \"config/UrbanElementsReID_test_reduced.yml\"\n",
    "competition_name = \"urban-reid-challenge\"\n",
    "submission_message = f\"test trained submission\"\n",
    "\n",
    "################ Probably nothing has to be modified from now on ################\n",
    "dataset_path = os.path.join('assets', 'datasets', competition_name)\n",
    "with open(config_train, 'r') as f:\n",
    "    hyperparams = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "model_path = os.path.join(hyperparams['LOG_ROOT'], hyperparams['LOG_NAME'])\n",
    "experiment_id: int = int(time.time())\n",
    "\n",
    "assert dotenv.load_dotenv('../../.env')\n",
    "assert os.getenv('KAGGLE_USERNAME')\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff13ad8-ef19-4a9d-b846-2baf9ff36605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aab73f9-7452-41a8-91f7-4a41bfb65c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_params_hash(params: Dict[str, Any]) -> str:\n",
    "    stringified = json.dumps({k: str(params[k]) for k in params}, sort_keys=True)\n",
    "    return hashlib.md5(stringified.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08beda4-3b09-408a-bec7-65e94d333d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Depednencies\n",
    "# TODO: move to .def\n",
    "!pip install torch torchvision torchaudio einops timm scikit-image opencv-python tensorboard yacs kaggle pyyaml\n",
    "\n",
    "# Download the dataset\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "    api.competition_download_files(competition_name, path=\"./assets\")\n",
    "    with zipfile.ZipFile(f'./assets/{competition_name}.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_path)\n",
    "    os.remove(f'./assets/{competition_name}.zip')\n",
    "    print(f\"Downloaded dataset for {competition_name}\")\n",
    "    \n",
    "    %cd assets/datasets/urban-reid-challenge\n",
    "    !mv ./image_query/image_query/* ./image_query/\n",
    "    !rm -r ./image_query/image_query\n",
    "    \n",
    "    !mv ./image_test/image_test/* ./image_test/\n",
    "    !rm -r ./image_test/image_test\n",
    "    \n",
    "    !mv ./image_train/image_train/* ./image_train/\n",
    "    !rm -r ./image_train/image_train\n",
    "    %cd ../../..\n",
    "else:\n",
    "    print(f\"Dataset already existed\")\n",
    "\n",
    "# Generate reduced dataset\n",
    "'''\n",
    "cp -r urban-reid-challenge/ urban-reid-challenge-reduced/\n",
    "cd urban-reid-challenge-reduced/\n",
    "for file in image_query/0*.jpg; do\n",
    "    num=$(basename \"$file\" .jpg)  # Extract number\n",
    "    num=$((10#$num))  # Convert to decimal\n",
    "    if ((num > 3)); then\n",
    "        rm \"$file\"\n",
    "    fi\n",
    "done\n",
    "for file in image_test/0*.jpg; do\n",
    "    num=$(basename \"$file\" .jpg)  # Extract number\n",
    "    num=$((10#$num))  # Convert to decimal\n",
    "    if ((num > 10)); then\n",
    "        rm \"$file\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "sed -i '4q' query.csv\n",
    "sed -i '11q' test.csv\n",
    "\n",
    "''';\n",
    "\n",
    "\n",
    "# Download the model\n",
    "os.makedirs('assets/models', exist_ok=True)\n",
    "if not os.path.exists('assets/models/resnet50-19c8e357.pth'):\n",
    "    !curl -o \"assets/models/resnet50-19c8e357.pth\" \"https://download.pytorch.org/models/resnet50-19c8e357.pth\"\n",
    "else:\n",
    "    print('model already existed')\n",
    "if not os.path.exists('assets/models/jx_vit_base_p16_224-80ecf9dd.pth'):\n",
    "    !curl -L -o 'assets/models/jx_vit_base_p16_224-80ecf9dd.pth'  'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth'\n",
    "else:\n",
    "    print('model already existed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039dfdd-40d4-4c81-a1f5-e44e123c1e20",
   "metadata": {},
   "source": [
    "# Train embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0513fd7c-1636-4bb5-8167-951880abf61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  4 02:20:49 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           On  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              25W / 250W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9b3a8cda-2764-4871-b155-f00a6260666f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-04 02:00:49,419 PAT INFO: Saving model in the path :assets/models/PAT\n",
      "2025-04-04 02:00:49,419 PAT INFO: Namespace(config_file='config/UrbanElementsReID_train.yml', opts=[], local_rank=0)\n",
      "2025-04-04 02:00:49,419 PAT INFO: Loaded configuration file config/UrbanElementsReID_train.yml\n",
      "2025-04-04 02:00:49,419 PAT INFO: \n",
      "MODEL:\n",
      "  PRETRAIN_CHOICE: 'imagenet'\n",
      "  #PRETRAIN_PATH: \"../../.cache/torch/hub/checkpoints\" # root of pretrain path\n",
      "  PRETRAIN_PATH: \"assets/models\" \n",
      "  IF_LABELSMOOTH: 'on'\n",
      "  IF_WITH_CENTER: 'no'\n",
      "  NAME: 'part_attention_vit'\n",
      "  NO_MARGIN: True\n",
      "  DEVICE_ID: ('0')\n",
      "  TRANSFORMER_TYPE: 'vit_base_patch16_224_TransReID'\n",
      "  STRIDE_SIZE: [16, 16]\n",
      "\n",
      "INPUT:\n",
      "  SIZE_TRAIN: [256,128]\n",
      "  SIZE_TEST: [256,128]\n",
      "  REA:\n",
      "    ENABLED: False\n",
      "  PIXEL_MEAN: [0.5, 0.5, 0.5]\n",
      "  PIXEL_STD: [0.5, 0.5, 0.5]\n",
      "  LGT: # Local Grayscale Transfomation\n",
      "    DO_LGT: True\n",
      "    PROB: 0.5\n",
      "\n",
      "DATASETS:\n",
      "  TRAIN: ('UrbanElementsReID',)\n",
      "  TEST: ('UrbanElementsReID',)\n",
      "  #ROOT_DIR: ('../../data') # root of datasets\n",
      "  #ROOT_DIR: '/home/jgf/Desktop/rhome/jgf/baselineChallenge/UrbanElementsReID/'\n",
      "  ROOT_DIR: 'assets/datasets/urban-reid-challenge'\n",
      "\n",
      "DATALOADER:\n",
      "  SAMPLER: 'softmax_triplet'\n",
      "  NUM_INSTANCE: 4\n",
      "  NUM_WORKERS: 8\n",
      "\n",
      "SOLVER:\n",
      "  OPTIMIZER_NAME: 'SGD'\n",
      "  MAX_EPOCHS: 1\n",
      "  BASE_LR: 0.001 # 0.0004 for msmt\n",
      "  IMS_PER_BATCH: 64\n",
      "  WARMUP_METHOD: 'linear'\n",
      "  LARGE_FC_LR: False\n",
      "  CHECKPOINT_PERIOD: 1\n",
      "  LOG_PERIOD: 1\n",
      "  EVAL_PERIOD: 1\n",
      "  WEIGHT_DECAY:  1e-4\n",
      "  WEIGHT_DECAY_BIAS: 1e-4\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  SEED: 1234\n",
      "\n",
      "TEST:\n",
      "  EVAL: True\n",
      "  IMS_PER_BATCH: 128\n",
      "  RE_RANKING: False\n",
      "  WEIGHT: ''\n",
      "  NECK_FEAT: 'before'\n",
      "  FEAT_NORM: True\n",
      "\n",
      "LOG_ROOT: 'assets/models/' # root of log file\n",
      "TB_LOG_ROOT: './assets/tb_log/'\n",
      "LOG_NAME: 'PAT'\n",
      "\n",
      "2025-04-04 02:00:49,420 PAT INFO: Running with config:\n",
      "DATALOADER:\n",
      "  CAMERA_TO_DOMAIN: False\n",
      "  DELETE_REM: False\n",
      "  DROP_LAST: False\n",
      "  INDIVIDUAL: False\n",
      "  NAIVE_WAY: True\n",
      "  NUM_INSTANCE: 4\n",
      "  NUM_WORKERS: 8\n",
      "  SAMPLER: softmax_triplet\n",
      "DATASETS:\n",
      "  COMBINEALL: False\n",
      "  ROOT_DIR: assets/datasets/urban-reid-challenge\n",
      "  TEST: ('UrbanElementsReID',)\n",
      "  TRAIN: ('UrbanElementsReID',)\n",
      "INPUT:\n",
      "  CJ:\n",
      "    BRIGHTNESS: 0.15\n",
      "    CONTRAST: 0.15\n",
      "    ENABLED: False\n",
      "    HUE: 0.1\n",
      "    PROB: 1.0\n",
      "    SATURATION: 0.1\n",
      "  DO_AUGMIX: False\n",
      "  DO_AUTOAUG: False\n",
      "  DO_FLIP: True\n",
      "  DO_PAD: True\n",
      "  FLIP_PROB: 0.5\n",
      "  LGT:\n",
      "    DO_LGT: True\n",
      "    PROB: 0.5\n",
      "  PADDING: 10\n",
      "  PADDING_MODE: constant\n",
      "  PIXEL_MEAN: [0.5, 0.5, 0.5]\n",
      "  PIXEL_STD: [0.5, 0.5, 0.5]\n",
      "  REA:\n",
      "    ENABLED: False\n",
      "    MEAN: [123.675, 116.28, 103.53]\n",
      "    PROB: 0.5\n",
      "  RPT:\n",
      "    ENABLED: False\n",
      "    PROB: 0.5\n",
      "  SIZE_TEST: [256, 128]\n",
      "  SIZE_TRAIN: [256, 128]\n",
      "LOG_NAME: PAT\n",
      "LOG_ROOT: assets/models/\n",
      "MODEL:\n",
      "  ATT_DROP_RATE: 0.0\n",
      "  CLUSTER_K: 10\n",
      "  COS_LAYER: False\n",
      "  DEVICE: cuda\n",
      "  DEVICE_ID: 0\n",
      "  DIST_TRAIN: False\n",
      "  DROP_OUT: 0.0\n",
      "  DROP_PATH: 0.1\n",
      "  FREEZE_PATCH_EMBED: True\n",
      "  ID_LOSS_TYPE: softmax\n",
      "  ID_LOSS_WEIGHT: 1.0\n",
      "  IF_LABELSMOOTH: on\n",
      "  IF_WITH_CENTER: no\n",
      "  LAST_STRIDE: 1\n",
      "  METRIC_LOSS_TYPE: triplet\n",
      "  NAME: part_attention_vit\n",
      "  NECK: bnneck\n",
      "  NO_MARGIN: True\n",
      "  PATCH_EMBED_TYPE: \n",
      "  PC_LOSS: True\n",
      "  PC_LR: 1.0\n",
      "  PC_SCALE: 0.02\n",
      "  PRETRAIN_CHOICE: imagenet\n",
      "  PRETRAIN_PATH: assets/models\n",
      "  SOFT_LABEL: True\n",
      "  SOFT_LAMBDA: 0.5\n",
      "  SOFT_WEIGHT: 0.5\n",
      "  STRIDE_SIZE: [16, 16]\n",
      "  TRANSFORMER_TYPE: vit_base_patch16_224_TransReID\n",
      "  TRIPLET_LOSS_WEIGHT: 1.0\n",
      "SOLVER:\n",
      "  BASE_LR: 0.001\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  CENTER_LOSS_WEIGHT: 0.0005\n",
      "  CENTER_LR: 0.5\n",
      "  CHECKPOINT_PERIOD: 1\n",
      "  COSINE_MARGIN: 0.5\n",
      "  COSINE_SCALE: 30\n",
      "  EVAL_PERIOD: 1\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 64\n",
      "  LARGE_FC_LR: False\n",
      "  LOG_PERIOD: 1\n",
      "  MARGIN: 0.3\n",
      "  MAX_EPOCHS: 1\n",
      "  MOMENTUM: 0.9\n",
      "  OPTIMIZER_NAME: SGD\n",
      "  SEED: 1234\n",
      "  STEPS: (40, 70)\n",
      "  WARMUP_EPOCHS: 5\n",
      "  WARMUP_FACTOR: 0.01\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: 0.0001\n",
      "TB_LOG_ROOT: ./assets/tb_log/\n",
      "TEST:\n",
      "  DIST_MAT: dist_mat.npy\n",
      "  EVAL: True\n",
      "  FEAT_NORM: True\n",
      "  IMS_PER_BATCH: 128\n",
      "  NECK_FEAT: before\n",
      "  RE_RANKING: False\n",
      "  WEIGHT: \n",
      "2025-04-04 02:00:49,449 PAT INFO: => Loaded UrbanElementsReID\n",
      "2025-04-04 02:00:49,449 PAT INFO:   ----------------------------------------\n",
      "2025-04-04 02:00:49,450 PAT INFO:   subset   | # ids | # images | # cameras\n",
      "2025-04-04 02:00:49,450 PAT INFO:   ----------------------------------------\n",
      "2025-04-04 02:00:49,450 PAT INFO:   train    |   288 |     3607 |         3\n",
      "2025-04-04 02:00:49,450 PAT INFO:   ----------------------------------------\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "????????????????? UrbanElementsReID assets/datasets/urban-reid-challenge\n",
      "2025-04-04 02:00:49,489 PAT INFO: => Loaded UrbanElementsReID\n",
      "2025-04-04 02:00:49,489 PAT INFO:   ----------------------------------------\n",
      "2025-04-04 02:00:49,489 PAT INFO:   subset   | # ids | # images | # cameras\n",
      "2025-04-04 02:00:49,489 PAT INFO:   ----------------------------------------\n",
      "2025-04-04 02:00:49,489 PAT INFO:   query    |   288 |     3607 |         3\n",
      "2025-04-04 02:00:49,490 PAT INFO:   gallery  |   288 |     3607 |         3\n",
      "2025-04-04 02:00:49,490 PAT INFO:   ----------------------------------------\n",
      "using Transformer_type: part token vit as a backbone\n",
      "using stride: [16, 16], and patch number is num_y16 * num_x8\n",
      "using drop_out rate is : 0.0\n",
      "using attn_drop_out rate is : 0.0\n",
      "using drop_path rate is : 0.1\n",
      "Resized position embedding from size:torch.Size([1, 197, 768]) to size: torch.Size([1, 132, 768]) with height:16 width: 8\n",
      "Load 153 / 155 layers.\n",
      "Loading pretrained ImageNet model......from assets/models/jx_vit_base_p16_224-80ecf9dd.pth\n",
      "===========building our part attention vit===========\n",
      "2025-04-04 02:00:51,360 PAT.train INFO: Number of parameter: 86.74M\n",
      "====== freeze patch_embed for stability ======\n",
      "using soft triplet loss for training\n",
      "label smooth on, numclasses: 288\n",
      "========using soft label========\n",
      "2025-04-04 02:00:51,562 PAT.train INFO: start training\n",
      "saving tblog to ./assets/tb_log/PAT\n",
      "/home/benle1/Urban-Elements-ReID-Challenge/Code/Files for PAT/processor/part_attention_vit_processor.py:52: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(init_scale=512)\n",
      "initialize the centers\n",
      "initialization done\n",
      "/home/benle1/Urban-Elements-ReID-Challenge/Code/Files for PAT/processor/part_attention_vit_processor.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=True):\n",
      "/home/benle1/Urban-Elements-ReID-Challenge/Code/Files for PAT/loss/myloss.py:29: UserWarning: This overload of addmm_ is deprecated:\n",
      "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta = 1, Number alpha = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1661.)\n",
      "  dist_map.addmm_(1, -2, part_feat, part_centers.t())\n",
      "2025-04-04 02:01:10,314 PAT.train INFO: Epoch[1] Iteration[1/51] total_loss: 12.333, reid_loss: 11.412, pc_loss: 0.920, Acc: 0.000, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:10,575 PAT.train INFO: Epoch[1] Iteration[2/51] total_loss: 11.815, reid_loss: 10.876, pc_loss: 0.938, Acc: 0.008, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:10,812 PAT.train INFO: Epoch[1] Iteration[3/51] total_loss: 11.679, reid_loss: 10.707, pc_loss: 0.972, Acc: 0.005, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:11,058 PAT.train INFO: Epoch[1] Iteration[4/51] total_loss: 11.475, reid_loss: 10.512, pc_loss: 0.963, Acc: 0.016, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:11,332 PAT.train INFO: Epoch[1] Iteration[5/51] total_loss: 11.441, reid_loss: 10.466, pc_loss: 0.975, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:11,623 PAT.train INFO: Epoch[1] Iteration[6/51] total_loss: 11.428, reid_loss: 10.437, pc_loss: 0.991, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:11,917 PAT.train INFO: Epoch[1] Iteration[7/51] total_loss: 11.323, reid_loss: 10.331, pc_loss: 0.993, Acc: 0.011, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:12,180 PAT.train INFO: Epoch[1] Iteration[8/51] total_loss: 11.417, reid_loss: 10.418, pc_loss: 0.999, Acc: 0.012, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:12,464 PAT.train INFO: Epoch[1] Iteration[9/51] total_loss: 11.370, reid_loss: 10.369, pc_loss: 1.001, Acc: 0.010, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:12,732 PAT.train INFO: Epoch[1] Iteration[10/51] total_loss: 11.277, reid_loss: 10.269, pc_loss: 1.008, Acc: 0.011, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:13,029 PAT.train INFO: Epoch[1] Iteration[11/51] total_loss: 11.215, reid_loss: 10.201, pc_loss: 1.014, Acc: 0.010, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:13,328 PAT.train INFO: Epoch[1] Iteration[12/51] total_loss: 11.139, reid_loss: 10.117, pc_loss: 1.021, Acc: 0.010, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:13,584 PAT.train INFO: Epoch[1] Iteration[13/51] total_loss: 11.093, reid_loss: 10.058, pc_loss: 1.035, Acc: 0.010, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:13,864 PAT.train INFO: Epoch[1] Iteration[14/51] total_loss: 11.128, reid_loss: 10.091, pc_loss: 1.037, Acc: 0.009, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:14,114 PAT.train INFO: Epoch[1] Iteration[15/51] total_loss: 11.074, reid_loss: 10.034, pc_loss: 1.039, Acc: 0.009, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:14,435 PAT.train INFO: Epoch[1] Iteration[16/51] total_loss: 11.031, reid_loss: 9.989, pc_loss: 1.042, Acc: 0.009, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:14,729 PAT.train INFO: Epoch[1] Iteration[17/51] total_loss: 11.047, reid_loss: 10.004, pc_loss: 1.043, Acc: 0.011, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:15,016 PAT.train INFO: Epoch[1] Iteration[18/51] total_loss: 11.052, reid_loss: 10.002, pc_loss: 1.051, Acc: 0.010, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:15,263 PAT.train INFO: Epoch[1] Iteration[19/51] total_loss: 11.029, reid_loss: 9.978, pc_loss: 1.052, Acc: 0.012, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:15,566 PAT.train INFO: Epoch[1] Iteration[20/51] total_loss: 11.029, reid_loss: 9.977, pc_loss: 1.052, Acc: 0.011, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:15,832 PAT.train INFO: Epoch[1] Iteration[21/51] total_loss: 11.028, reid_loss: 9.978, pc_loss: 1.050, Acc: 0.011, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:16,091 PAT.train INFO: Epoch[1] Iteration[22/51] total_loss: 11.019, reid_loss: 9.964, pc_loss: 1.055, Acc: 0.011, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:16,425 PAT.train INFO: Epoch[1] Iteration[23/51] total_loss: 11.093, reid_loss: 10.035, pc_loss: 1.058, Acc: 0.011, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:16,704 PAT.train INFO: Epoch[1] Iteration[24/51] total_loss: 11.093, reid_loss: 10.029, pc_loss: 1.064, Acc: 0.011, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:16,992 PAT.train INFO: Epoch[1] Iteration[25/51] total_loss: 11.103, reid_loss: 10.039, pc_loss: 1.064, Acc: 0.012, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:17,270 PAT.train INFO: Epoch[1] Iteration[26/51] total_loss: 11.067, reid_loss: 10.002, pc_loss: 1.064, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:17,566 PAT.train INFO: Epoch[1] Iteration[27/51] total_loss: 11.058, reid_loss: 9.988, pc_loss: 1.069, Acc: 0.012, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:17,820 PAT.train INFO: Epoch[1] Iteration[28/51] total_loss: 11.047, reid_loss: 9.975, pc_loss: 1.073, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:18,095 PAT.train INFO: Epoch[1] Iteration[29/51] total_loss: 11.045, reid_loss: 9.966, pc_loss: 1.078, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:18,397 PAT.train INFO: Epoch[1] Iteration[30/51] total_loss: 11.023, reid_loss: 9.940, pc_loss: 1.082, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:18,663 PAT.train INFO: Epoch[1] Iteration[31/51] total_loss: 11.008, reid_loss: 9.922, pc_loss: 1.086, Acc: 0.014, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:18,917 PAT.train INFO: Epoch[1] Iteration[32/51] total_loss: 11.004, reid_loss: 9.919, pc_loss: 1.085, Acc: 0.014, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:19,173 PAT.train INFO: Epoch[1] Iteration[33/51] total_loss: 11.017, reid_loss: 9.930, pc_loss: 1.088, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:19,445 PAT.train INFO: Epoch[1] Iteration[34/51] total_loss: 11.014, reid_loss: 9.925, pc_loss: 1.089, Acc: 0.014, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:19,657 PAT.train INFO: Epoch[1] Iteration[35/51] total_loss: 10.991, reid_loss: 9.897, pc_loss: 1.094, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:19,865 PAT.train INFO: Epoch[1] Iteration[36/51] total_loss: 10.999, reid_loss: 9.904, pc_loss: 1.095, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:20,071 PAT.train INFO: Epoch[1] Iteration[37/51] total_loss: 10.974, reid_loss: 9.880, pc_loss: 1.094, Acc: 0.014, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:20,279 PAT.train INFO: Epoch[1] Iteration[38/51] total_loss: 10.976, reid_loss: 9.882, pc_loss: 1.094, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:20,485 PAT.train INFO: Epoch[1] Iteration[39/51] total_loss: 10.962, reid_loss: 9.865, pc_loss: 1.097, Acc: 0.014, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:20,694 PAT.train INFO: Epoch[1] Iteration[40/51] total_loss: 10.969, reid_loss: 9.868, pc_loss: 1.101, Acc: 0.013, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:20,900 PAT.train INFO: Epoch[1] Iteration[41/51] total_loss: 10.948, reid_loss: 9.845, pc_loss: 1.103, Acc: 0.015, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:21,108 PAT.train INFO: Epoch[1] Iteration[42/51] total_loss: 10.943, reid_loss: 9.839, pc_loss: 1.103, Acc: 0.016, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:21,321 PAT.train INFO: Epoch[1] Iteration[43/51] total_loss: 10.917, reid_loss: 9.812, pc_loss: 1.105, Acc: 0.016, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:21,528 PAT.train INFO: Epoch[1] Iteration[44/51] total_loss: 10.909, reid_loss: 9.801, pc_loss: 1.108, Acc: 0.017, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:21,738 PAT.train INFO: Epoch[1] Iteration[45/51] total_loss: 10.934, reid_loss: 9.824, pc_loss: 1.111, Acc: 0.021, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:21,944 PAT.train INFO: Epoch[1] Iteration[46/51] total_loss: 10.924, reid_loss: 9.808, pc_loss: 1.115, Acc: 0.021, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:22,152 PAT.train INFO: Epoch[1] Iteration[47/51] total_loss: 10.914, reid_loss: 9.796, pc_loss: 1.118, Acc: 0.021, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:22,360 PAT.train INFO: Epoch[1] Iteration[48/51] total_loss: 10.907, reid_loss: 9.784, pc_loss: 1.122, Acc: 0.021, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:22,566 PAT.train INFO: Epoch[1] Iteration[49/51] total_loss: 10.928, reid_loss: 9.804, pc_loss: 1.124, Acc: 0.023, Base Lr: 2.08e-04\n",
      "2025-04-04 02:01:22,675 PAT.train INFO: Epoch 1 done. Time per batch: 0.337[s] Speed: 190.1[samples/s]\n",
      "2025-04-04 02:01:22,675 PAT.test INFO: Enter inferencing\n",
      "2025-04-04 02:01:48,677 PAT.test INFO: Validation Results \n",
      "2025-04-04 02:01:48,678 PAT.test INFO: mAP: 15.4%\n",
      "2025-04-04 02:01:48,678 PAT.test INFO: CMC curve, Rank-1  :28.9%\n",
      "2025-04-04 02:01:48,678 PAT.test INFO: CMC curve, Rank-5  :52.3%\n",
      "2025-04-04 02:01:48,678 PAT.test INFO: CMC curve, Rank-10 :63.8%\n",
      "2025-04-04 02:01:48,678 PAT.test INFO: total inference time: 26.00\n",
      "2025-04-04 02:01:48,682 PAT.train INFO: =====best epoch: 1=====\n",
      "using Transformer_type: part token vit as a backbone\n",
      "using stride: [16, 16], and patch number is num_y16 * num_x8\n",
      "using drop_out rate is : 0.0\n",
      "using attn_drop_out rate is : 0.0\n",
      "using drop_path rate is : 0.1\n",
      "Resized position embedding from size:torch.Size([1, 197, 768]) to size: torch.Size([1, 132, 768]) with height:16 width: 8\n",
      "Load 153 / 155 layers.\n",
      "Loading pretrained ImageNet model......from assets/models/jx_vit_base_p16_224-80ecf9dd.pth\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "===========building our part attention vit===========\n",
      "2025-04-04 02:01:51,194 PAT.train INFO: Number of parameter: 86.52M\n",
      "Loading trained model from assets/models/PAT/part_attention_vit_1.pth\n",
      "load weights from part_attention_vit_1.pth\n",
      "????????????????? UrbanElementsReID assets/datasets/urban-reid-challenge\n",
      "2025-04-04 02:01:51,640 PAT INFO: => Loaded UrbanElementsReID\n",
      "2025-04-04 02:01:51,640 PAT INFO:   ----------------------------------------\n",
      "2025-04-04 02:01:51,640 PAT INFO:   subset   | # ids | # images | # cameras\n",
      "2025-04-04 02:01:51,640 PAT INFO:   ----------------------------------------\n",
      "2025-04-04 02:01:51,640 PAT INFO:   query    |   288 |     3607 |         3\n",
      "2025-04-04 02:01:51,640 PAT INFO:   gallery  |   288 |     3607 |         3\n",
      "2025-04-04 02:01:51,640 PAT INFO:   ----------------------------------------\n",
      "2025-04-04 02:01:51,640 PAT.test INFO: Enter inferencing\n",
      "2025-04-04 02:02:19,116 PAT.test INFO: Validation Results \n",
      "2025-04-04 02:02:19,117 PAT.test INFO: mAP: 15.3%\n",
      "2025-04-04 02:02:19,117 PAT.test INFO: CMC curve, Rank-1  :28.8%\n",
      "2025-04-04 02:02:19,117 PAT.test INFO: CMC curve, Rank-5  :52.9%\n",
      "2025-04-04 02:02:19,117 PAT.test INFO: CMC curve, Rank-10 :64.0%\n",
      "2025-04-04 02:02:19,117 PAT.test INFO: total inference time: 27.39\n",
      "removing assets/models/PAT/part_attention_vit_1.pth. \n",
      "saving final checkpoint.\n",
      "Do not interrupt the program!!!\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists('assets'), 'are you are in the right folder?'\n",
    "assert os.getcwd().endswith('PAT'), 'are you are in the right folder?'\n",
    "if os.path.exists(model_path):\n",
    "    shutil.rmtree(model_path)\n",
    "!python train.py --config_file {config_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "754e830d-d112-464d-b692-941ff917a047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epoch: 1\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(os.path.join(model_path, \"part_attention_vit_*.pth\"))\n",
    "max_epoch = max([int(f.split('_')[-1].split('.')[0]) for f in files])\n",
    "assert max_epoch > 0\n",
    "assert os.path.exists(os.path.join(model_path, f'part_attention_vit_{max_epoch}.pth'))\n",
    "print('max_epoch:', max_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138e441-7471-4063-afb5-f31974870dfa",
   "metadata": {},
   "source": [
    "# Refine query images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be0983-632d-4eba-a976-ef1931f83518",
   "metadata": {},
   "source": [
    "for each image, generate refinements following naming like:\n",
    "000001_refinement_A\n",
    "000002_refinement_B\n",
    "000002_refinement_C\n",
    "\n",
    "save them in urban-reid-challenge-reduced/image_query/\n",
    "\n",
    "Add those entires to urban-reid-challenge-reduced/image_query/query.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5cf707-31e1-43c4-8131-a90d06c6360d",
   "metadata": {},
   "source": [
    "# Generate ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e4b060-5b5a-4705-a8d2-bfcc366ef67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_test, 'r') as f:\n",
    "    hyperparams_test = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "assert hyperparams_test['TEST']['WEIGHT'] == os.path.join(model_path, f'part_attention_vit_{max_epoch}.pth'), 'not testing with the trained model...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "022e41fb-d39d-49ee-b3d4-1b3494c2d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python update.py --config_file {config_test} --track {os.path.join(model_path, \"track.txt\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3397a82e-b465-42a6-a3d8-deb7bf395fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_name = os.path.join(model_path, \"track_submission.csv\")\n",
    "assert os.path.exists(submission_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e343db-277b-448c-b75e-e25a1f6037c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r {model_path} {model_path}_backup_{experiment_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2dd26-edd5-46b2-a7d7-f9c99c87856c",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "87d10dd9-d401-4ea1-9080-7a4aee95fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(submission_file_name)\n",
    "df = pd.read_csv(submission_file_name)\n",
    "assert df.shape[1] == 2\n",
    "assert df.shape[0] > 100\n",
    "\n",
    "submission_message += f\"; commit_hash: {subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('ascii').strip()}\"\n",
    "submission_message += f\"; hyperparameters_hash: {calculate_params_hash(hyperparams)}\"\n",
    "submission_message += f\"; experiment_ID: {experiment_id}\"\n",
    "print(f'Submitting with message \"{submission_message}\"')\n",
    "\n",
    "# Submit the file to the competition\n",
    "# Uncomment only for actual submissions!\n",
    "#api.competition_submit(submission_file_name, submission_message, competition_name)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
