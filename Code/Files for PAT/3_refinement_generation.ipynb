{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8be2e4-990e-43eb-bd9e-beb41fab8af7",
   "metadata": {},
   "source": [
    "Reads the file `query_with_refinements.csv`\n",
    "\n",
    "Generate the images, save them to `image_query/`\n",
    "\n",
    "Overwrites the file `query.csv` (a backup of the past file is saved to `query_backup.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604abdfd-418e-40f0-9dea-9dc134489038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import logging\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "from openai.error import RateLimitError\n",
    "import base64\n",
    "import io\n",
    "import dotenv\n",
    "import time\n",
    "import shutil\n",
    "from typing import Optional\n",
    "import logging\n",
    "from huggingface_hub import HfApi, snapshot_download\n",
    "import torch\n",
    "from diffusers.utils import load_image\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "dataset = 'urban-reid-challenge'\n",
    "\n",
    "################ Probably nothing has to be modified from now on ################\n",
    "logging.root.setLevel(logging.INFO)\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "sys.path.append('assets/libs/diffusion-self-distillation')\n",
    "assert os.path.exists('assets/libs/diffusion-self-distillation/')\n",
    "\n",
    "from transformer import FluxTransformer2DConditionalModel\n",
    "from pipeline import FluxConditionalPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61674c41-9d0f-4553-abb4-4122992b28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityDiffuserDSD():\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,                   # Path to the 'transformer' folder\n",
    "        lora_path: str,                    # Path to the 'pytorch_lora_weights.safetensors' file\n",
    "        guidance: float = 3.5,             # Guidance scale\n",
    "        i_guidance: float = 1.0,           # True image guidance scale, set to >1.0 if you want to enhance the image conditioning\n",
    "        t_guidance: float = 1.0,           # True text guidance scale, set to >1.0 if you want to enhance the text conditioning\n",
    "        model_offload: bool = False,       # Enable basic model offloading to CPU to reduce GPU memory usage (recommended, requires ~23.7GB VRAM)\n",
    "        sequential_offload: bool = False,  # Enable more aggressive sequential offloading (saves more memory but much slower, requires < 1GB VRAM)\n",
    "        steps: int = 28,\n",
    "    ):\n",
    "        transformer = FluxTransformer2DConditionalModel.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "        pipe = FluxConditionalPipeline.from_pretrained(\n",
    "            \"black-forest-labs/FLUX.1-dev\",\n",
    "            transformer=transformer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        assert isinstance(pipe, FluxConditionalPipeline)\n",
    "        pipe.load_lora_weights(lora_path)\n",
    "        if model_offload:\n",
    "            pipe.enable_model_cpu_offload()\n",
    "        if sequential_offload:\n",
    "            pipe.enable_sequential_cpu_offload()\n",
    "        if not model_offload and not sequential_offload:\n",
    "            pipe.to(\"cuda\")\n",
    "\n",
    "        self.guidance = guidance\n",
    "        self.i_guidance = i_guidance\n",
    "        self.t_guidance = t_guidance\n",
    "        self.steps = steps\n",
    "        self.pipe = pipe\n",
    "\n",
    "    # Open the image\n",
    "    def predict(\n",
    "        self,\n",
    "        image_path: str,\n",
    "        output_path: str,\n",
    "        text: str,\n",
    "    ):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        print(f\"Process image: {image_path}\")\n",
    "\n",
    "        # Process image and text\n",
    "        result_image = self.process_image_and_text(\n",
    "            self.pipe,\n",
    "            image,\n",
    "            text,\n",
    "            self.guidance,\n",
    "            self.i_guidance,\n",
    "            self.t_guidance,\n",
    "            self.steps\n",
    "        )\n",
    "\n",
    "        # Save the output\n",
    "        result_image.save(output_path)\n",
    "        print(f\"Output saved to {output_path}\")\n",
    "\n",
    "    def process_image_and_text(\n",
    "        self,\n",
    "        pipe,\n",
    "        image,\n",
    "        text,\n",
    "        guidance,\n",
    "        i_guidance,\n",
    "        t_guidance,\n",
    "        steps,\n",
    "    ):\n",
    "        \"\"\"Process the given image and text using the global pipeline.\"\"\"\n",
    "        # center-crop image\n",
    "        w, h = image.size\n",
    "        min_size = min(w, h)\n",
    "        image = image.crop(\n",
    "            (\n",
    "                (w - min_size) // 2,\n",
    "                (h - min_size) // 2,\n",
    "                (w + min_size) // 2,\n",
    "                (h + min_size) // 2,\n",
    "            )\n",
    "        )\n",
    "        image = image.resize((512, 512))\n",
    "\n",
    "        control_image = load_image(image)\n",
    "        result = pipe(\n",
    "            prompt=text.strip().replace(\"\\n\", \"\").replace(\"\\r\", \"\"),\n",
    "            negative_prompt=\"\",\n",
    "            num_inference_steps=steps,\n",
    "            height=512,\n",
    "            width=1024,\n",
    "            guidance_scale=guidance,\n",
    "            image=control_image,\n",
    "            guidance_scale_real_i=i_guidance,\n",
    "            guidance_scale_real_t=t_guidance,\n",
    "        ).images[0]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def zip_folder(folder_path, zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                abs_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(abs_path, folder_path)\n",
    "                zipf.write(abs_path, arcname=rel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef88b9-e3b4-4967-a354-ebe325b74297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'assets/datasets/{dataset}/query_with_refinements.csv')\n",
    "\n",
    "# TODO: warning \"Some weights of the model checkpoint were not used when initializing FluxTransformer2DConditionalModel\"\n",
    "# Is that ok?\n",
    "model = IdentityDiffuserDSD(\n",
    "    model_path=\"assets/models/dsd_pretrained/transformer\",\n",
    "    lora_path=\"assets/models/dsd_pretrained/pytorch_lora_weights.safetensors\",\n",
    "    guidance=3.5,\n",
    "    i_guidance=1.0,\n",
    "    t_guidance=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d7d62-0f7f-4f70-9a39-91c9921be203",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "print(f'Generating images')\n",
    "for index, row in df[df['imageName'].str.contains('_refinement_')].iterrows():\n",
    "    output_path = os.path.join(f'assets/datasets/{dataset}/image_query', row['imageName'])\n",
    "\n",
    "    model.predict(\n",
    "        image_path=output_path.replace('_refinement', '').replace('_A.', '.').replace('_B.', '.').replace('_C.', '.'),\n",
    "        output_path=output_path,\n",
    "        text=row['description'],\n",
    "    )\n",
    "\n",
    "    plt.imshow(plt.imread(output_path))\n",
    "    plt.title(row['imageName'])\n",
    "    plt.show()\n",
    "    if index%50 == 0:  # TODO: this counting is not right, index doesn't represent the number of generated images\n",
    "        logging.info(f'Generated {index} images in {(time.time() - t0)} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939dc28e-d556-4ff1-96b9-138b9fd24ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overwrites the file `query.csv`\n",
    "shutil.copy2(\n",
    "    src=f'assets/datasets/{dataset}/query.csv',\n",
    "    dst=f'assets/datasets/{dataset}/query_backup.csv',\n",
    ")\n",
    "df.drop(['class', 'description'], axis=1).to_csv(f'assets/datasets/{dataset}/query.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda080b3-ff6c-4b1c-9ca8-bc8d5c6e64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "zip_folder(f'assets/datasets/{dataset}/image_query/', 'assets/image_query.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431488a0-066a-448a-8bf2-81160f0b35af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
