{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fa01f-745b-4cc0-accd-1d9c642b27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import re\n",
    "import subprocess\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Literal, List, Optional, Tuple\n",
    "import dotenv\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import glob\n",
    "import hashlib\n",
    "import shutil\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from config import cfg\n",
    "from yacs.config import CfgNode\n",
    "from model import make_model\n",
    "from utils.re_ranking import re_ranking\n",
    "from data.build_DG_dataloader import build_reid_test_loader\n",
    "from processor.ori_vit_processor_with_amp import do_inference as do_inf\n",
    "from processor.part_attention_vit_processor import do_inference as do_inf_pat\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.dinov2 import Dinov2Model\n",
    "\n",
    "config_train_b = \"config/UrbanElementsReID_train.yml\"\n",
    "config_test_b = \"config/UrbanElementsReID_test.yml\"\n",
    "\n",
    "config_train_embedding_r = \"config/UrbanElementsReID_train_embedding_r.yml\"\n",
    "config_test_embedding_r = \"config/UrbanElementsReID_test_embedding_r.yml\"\n",
    "\n",
    "competition_name = \"urban-reid-challenge\"\n",
    "#submission_message = f\"Concatenation base+A+text, weights 3-2-1\"\n",
    "submission_message = f\"New code\"\n",
    "experiment_id: int = int(time.time())\n",
    "number_of_refinements: int = 3\n",
    "base_multiplier = 3 # 1=base have the same value as each refinememnt; base_multiplier=number_of_refinements the base will have the same importance as all the refinements _combined_\n",
    "embedding_size: int = 768\n",
    "embedding_size_text: int = 768\n",
    "\n",
    "# The combination of these define how the fusion is made\n",
    "# Not all combinations are valid\n",
    "matching_use_refinements: bool = True\n",
    "matching_use_text: bool = False\n",
    "matching_use_percentile_fusion: Optional[float] = None  # bottom X%; if between 0 and 100, use that percentile; if none, use simple average, only used if `matching_use_refinements and not matching_use_text`\n",
    "embeddings_refinements_use_dino: bool = False  # If true, PAT is used for base iamges and Dino for refined; otherwise PAT is used for everything; Only aplicable if matching_use_refinements==True\n",
    "\n",
    "\n",
    "################ Probably nothing has to be modified from now on ################\n",
    "with open(config_train_b, 'r') as f:\n",
    "    hyperparams_train_b = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "with open(config_test_b, 'r') as f:\n",
    "    hyperparams_test_b = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "model_path_b = os.path.join(hyperparams_train_b['LOG_ROOT'], hyperparams_train_b['LOG_NAME'])\n",
    "\n",
    "with open(config_train_embedding_r, 'r') as f:\n",
    "    hyperparams_train_embedding_r = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "with open(config_test_embedding_r, 'r') as f:\n",
    "    hyperparams_test_embedding_r = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "model_path_embedding_r = os.path.join(hyperparams_train_embedding_r['LOG_ROOT'], hyperparams_train_embedding_r['LOG_NAME'])\n",
    "\n",
    "assert dotenv.load_dotenv('../../.env')\n",
    "assert os.getenv('KAGGLE_USERNAME')\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logging.warning(\"Where is your GPU dude?\")\n",
    "\n",
    "max_epoch = int(hyperparams_train_b['SOLVER']['MAX_EPOCHS'])\n",
    "assert hyperparams_test_b['TEST']['WEIGHT'].split('/')[-1] == f'part_attention_vit_{max_epoch}.pth', 'not testing with the trained model...'\n",
    "\n",
    "re_ranking_k1: int = 10 if 'reduced' in config_test_b  else 20\n",
    "num_gallery_base = sum(1 for f in Path(hyperparams_test_b['DATASETS']['ROOT_DIR']+'/image_test').iterdir() if f.is_file() and 'refinement' not in f.name.lower())  # Expected number of images in the galary\n",
    "submission_file_name = os.path.join(model_path_b, \"track_submission.csv\")\n",
    "track: str = os.path.join(model_path_b, \"track.txt\")\n",
    "dataset: str = competition_name  # That is not necessarily true, dataset must be taken form the hyperparams\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc1e795-e267-428b-b415-5d3d2ffbbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python update.py --config_file {config_test_b} --track {os.path.join(model_path_b, \"track.txt\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51dc32e-29d6-44a1-bfe7-c0f7db0d1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_pat(config: str) -> Tuple[CfgNode, nn.Module]:\n",
    "    cfg.merge_from_file(config)\n",
    "    cfg.freeze()\n",
    "    \n",
    "    output_dir = os.path.join(cfg.LOG_ROOT, cfg.LOG_NAME)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    logging.info(\"Loaded configuration file {}\".format(config))\n",
    "    with open(config, 'r') as cf:\n",
    "        config_str = \"\\n\" + cf.read()\n",
    "        logging.info(config_str)\n",
    "    logging.info(\"Running with config:\\n{}\".format(cfg))\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = cfg.MODEL.DEVICE_ID\n",
    "    \n",
    "    model = make_model(cfg, cfg.MODEL.NAME, 0,0,0)\n",
    "    model.load_param(cfg.TEST.WEIGHT)\n",
    "    return cfg, model\n",
    "\n",
    "def extract_feature_pat(model: nn.Module, dataloaders: DataLoader, subset: Literal['query', 'gallery'], filename_pattern: str = r'\\.') -> np.ndarray:\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: resnet featurizer\n",
    "        dataloaders: for everything\n",
    "        subset: to which suibset should the featurization should be restricted\n",
    "        filename_pattern: matches filename with `re.search`. The default value includes all images\n",
    "    Return:\n",
    "        feature vector shape [<number of images in subset, 768]\n",
    "    '''\n",
    "    # TODO: merge with extract_feature_dino\n",
    "    assert isinstance(model, nn.Module)\n",
    "    assert isinstance(dataloaders, DataLoader)\n",
    "    assert type(num_query) == int\n",
    "    with torch.no_grad():\n",
    "        features = torch.FloatTensor(0, 768).cuda()\n",
    "        count = 0\n",
    "        img_path = []\n",
    "        for data in dataloaders:\n",
    "            images, _, _, filenames, metadatas = data.values()\n",
    "            #print('>>>>>>>>>>>', images.shape)\n",
    "            #print('filenames 1', filenames)\n",
    "\n",
    "            # Select only the images that belong to the desired subset\n",
    "            subsets: List[Literal['query', 'gallery']] = metadatas['q_or_g']\n",
    "            mask = torch.tensor([s == subset for s in subsets], dtype=torch.bool)\n",
    "            images = images[mask]\n",
    "            filenames = list(np.array(filenames)[mask])\n",
    "            assert len(images) == sum(1 for s in subsets if s == subset)\n",
    "            #print('filenames 2', filenames)\n",
    "\n",
    "            # Select only the images that match the filename pattern\n",
    "            mask = torch.tensor([bool(re.search(filename_pattern, fn)) for fn in filenames], dtype=torch.bool)\n",
    "            images = images[mask]\n",
    "            filenames = list(np.array(filenames)[mask])\n",
    "            assert len(images) == sum(mask), \"Selection count mismatch\"\n",
    "\n",
    "            n, c, h, w = images.size()\n",
    "            if n==0:\n",
    "                continue\n",
    "\n",
    "            # This is the only PAT-specific part\n",
    "            count += n\n",
    "            ff = torch.FloatTensor(n, 768).zero_().cuda()  # 2048 is pool5 of resnet\n",
    "            for i in range(2):\n",
    "                input_img = images.cuda()\n",
    "                outputs = model(input_img)\n",
    "                f = outputs.float()\n",
    "                ff = ff + f\n",
    "            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "            ff = ff.div(fnorm.expand_as(ff))\n",
    "\n",
    "            features = torch.cat([features, ff], 0)\n",
    "            \n",
    "        assert features.shape[1] == 768\n",
    "        return features.cpu().numpy()\n",
    "\n",
    "\n",
    "def extract_feature_dino(model: Dinov2Model, dataloaders: DataLoader, subset: Literal['query', 'gallery'], filename_pattern: str = r'\\.') -> np.ndarray:\n",
    "    assert isinstance(model, Dinov2Model)\n",
    "    assert isinstance(dataloaders, DataLoader)\n",
    "    assert type(num_query) == int\n",
    "    with torch.no_grad():\n",
    "        features = torch.FloatTensor(0, 768).cuda()\n",
    "        count = 0\n",
    "        img_path = []\n",
    "        for data in dataloaders:\n",
    "            images, _, _, filenames, metadatas = data.values()\n",
    "            #print('>>>>>>>>>>>', images.shape)\n",
    "            #print('filenames 1', filenames)\n",
    "\n",
    "            # Select only the images that belong to the desired subset\n",
    "            subsets: List[Literal['query', 'gallery']] = metadatas['q_or_g']\n",
    "            mask = torch.tensor([s == subset for s in subsets], dtype=torch.bool)\n",
    "            images = images[mask]\n",
    "            filenames = list(np.array(filenames)[mask])\n",
    "            assert len(images) == sum(1 for s in subsets if s == subset)\n",
    "            #print('filenames 2', filenames)\n",
    "\n",
    "            # Select only the images that match the filename pattern\n",
    "            mask = torch.tensor([bool(re.search(filename_pattern, fn)) for fn in filenames], dtype=torch.bool)\n",
    "            images = images[mask]\n",
    "            filenames = list(np.array(filenames)[mask])\n",
    "            assert len(images) == sum(mask), \"Selection count mismatch\"\n",
    "\n",
    "            n, c, h, w = images.size()\n",
    "            if n==0:\n",
    "                continue\n",
    "\n",
    "            ######\n",
    "            # This is the only Dino-specific part\n",
    "            images = images.to(device)\n",
    "            batch_resized = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
    "            std  = torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
    "            batch_normalized = (batch_resized - mean) / std\n",
    "            \n",
    "            # Forward pass through DINOv2\n",
    "            out = model(pixel_values=batch_normalized)\n",
    "            ff = out.last_hidden_state[:, 0]  # CLS token, shape [n, 768]\n",
    "    \n",
    "            # L2 normalization\n",
    "            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "            ff = ff / fnorm\n",
    "\n",
    "            features = torch.cat([features, ff], dim=0)\n",
    "            \n",
    "        assert features.shape[1] == 768\n",
    "        return features.cpu().numpy()\n",
    "\n",
    "\n",
    "def extract_feature_text(model: SentenceTransformer, dataset: str, split: str) -> np.ndarray:\n",
    "    # Right now there isn't any filename_filtering because we read the csv that only contain the base images\n",
    "    # If we want to embbed the description of each refinement, then this has to be refactored\n",
    "    # Return array shape <num_images, embedding_size_text>\n",
    "    df = pd.read_csv(f'assets/datasets/{dataset}/{split}_with_description.csv')\n",
    "    logging.debug(df.head())\n",
    "    \n",
    "    X: List[str] = []\n",
    "    for index, row in df.iterrows():\n",
    "        X.append(f\"This is a {row['class']}. {row['description']}\")\n",
    "    assert len(X) == df.shape[0]\n",
    "    embeddings = model.encode(X)\n",
    "    assert embeddings.shape[0] == df.shape[0]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def l2_norm(x, axis=1, eps=1e-12):\n",
    "    norm = np.linalg.norm(x, ord=2, axis=axis, keepdims=True)\n",
    "    return x / (norm + eps)\n",
    "\n",
    "\n",
    "def calculate_params_hash(params: Dict[str, Any]) -> str:\n",
    "    stringified = json.dumps({k: str(params[k]) for k in params}, sort_keys=True)\n",
    "    return hashlib.md5(stringified.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ecafc-d67d-416e-91ef-1c41a020d53f",
   "metadata": {},
   "source": [
    "# Load things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16856e47-3859-444b-8738-c7249e1fef59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load models\n",
    "# TODO: idk why but after I trained PAT_r, the PAT folder was empty... maybe there is a bug and it was deleted somehow\n",
    "#!cp assets/models/PAT_backup_1746449878/* assets/models/PAT/\n",
    "\n",
    "\n",
    "if matching_use_refinements:\n",
    "    if embeddings_refinements_use_dino:\n",
    "        model_dino = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device).eval()\n",
    "    else:\n",
    "        cfg, model_pat_r = load_model_pat(config_test_embedding_r)\n",
    "\n",
    "cfg, model_pat_b = load_model_pat(config_test_b)\n",
    "\n",
    "if matching_use_text:\n",
    "    model_text = SentenceTransformer(\"all-mpnet-base-v2\")  # Load https://huggingface.co/sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf3d36f-0c6f-42d1-b69f-e84b19a35848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "for testname in cfg.DATASETS.TEST:\n",
    "    logging.info(f'>>>>>>>>>>>>>>>>>>>>>>> {testname}')\n",
    "    val_loader, num_query = build_reid_test_loader(cfg, testname)\n",
    "    \n",
    "    do_inf_pat(cfg, model_pat_b, val_loader, num_query)\n",
    "    if matching_use_refinements:\n",
    "        if embeddings_refinements_use_dino:\n",
    "            pass\n",
    "        else:\n",
    "            do_inf_pat(cfg, model_pat_r, val_loader, num_query)\n",
    "\n",
    "    #logging.info(type(model_pat_b))\n",
    "    #logging.info(type(val_loader))\n",
    "    #logging.info(type(num_query))\n",
    "num_query_base = int(num_query/(1+number_of_refinements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fef8ef-3be7-4ca7-b8ff-0c60ea785ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_query_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b75b1-417c-4e28-8a00-13e59569163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c513d5-896d-4b01-9a9f-5ec2393c64a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a575240-0104-47fb-94ac-b3a2250dcb92",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6bd62-a270-4a66-a8f4-b68694685288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from images\n",
    "qf_base = extract_feature_pat(model_pat_b, val_loader, subset='query', filename_pattern = r'^(?!.*refinement).*$')\n",
    "gf_base = extract_feature_pat(model_pat_b, val_loader, subset='gallery', filename_pattern = r'^(?!.*refinement).*$')\n",
    "\n",
    "logging.info(f'qf_base = {qf_base.shape}')\n",
    "logging.info(f'gf_base = {gf_base.shape}')\n",
    "assert qf_base.shape[0] == num_query_base\n",
    "assert qf_base.shape[1] == embedding_size\n",
    "assert gf_base.shape[0] == num_gallery_base\n",
    "assert gf_base.shape[1] == embedding_size\n",
    "#np.save(\"./qf.npy\", qf_base)\n",
    "#np.save(\"./gf.npy\", gf_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f047f4-fba3-4718-b3c7-d4c86de96664",
   "metadata": {},
   "outputs": [],
   "source": [
    "if matching_use_refinements:\n",
    "    if embeddings_refinements_use_dino:\n",
    "        qf_A = extract_feature_dino(model_dino, val_loader, subset='query', filename_pattern = r'_refinement_A\\.')\n",
    "        qf_B = extract_feature_dino(model_dino, val_loader, subset='query', filename_pattern = r'_refinement_B\\.')\n",
    "        qf_C = extract_feature_dino(model_dino, val_loader, subset='query', filename_pattern = r'_refinement_C\\.')\n",
    "        gf_A = extract_feature_dino(model_dino, val_loader, subset='gallery', filename_pattern = r'_refinement_A\\.')\n",
    "    else:\n",
    "        qf_A = extract_feature_pat(model_pat_r, val_loader, subset='query', filename_pattern = r'_refinement_A\\.')\n",
    "        qf_B = extract_feature_pat(model_pat_r, val_loader, subset='query', filename_pattern = r'_refinement_B\\.')\n",
    "        qf_C = extract_feature_pat(model_pat_r, val_loader, subset='query', filename_pattern = r'_refinement_C\\.')\n",
    "        gf_A = extract_feature_pat(model_pat_r, val_loader, subset='gallery', filename_pattern = r'_refinement_A\\.')\n",
    "    \n",
    "    logging.info(f'qf_A = {qf_A.shape}')\n",
    "    logging.info(f'qf_B = {qf_B.shape}')\n",
    "    logging.info(f'qf_C = {qf_C.shape}')\n",
    "    logging.info(f'gf_A = {gf_A.shape}')\n",
    "    assert qf_A.shape[0] == num_query_base\n",
    "    assert qf_A.shape[1] == embedding_size\n",
    "    assert qf_B.shape[0] == num_query_base\n",
    "    assert qf_B.shape[1] == embedding_size\n",
    "    assert qf_C.shape[0] == num_query_base\n",
    "    assert qf_C.shape[1] == embedding_size\n",
    "    assert gf_A.shape[0] == num_gallery_base\n",
    "    assert gf_A.shape[1] == embedding_size\n",
    "\n",
    "#############################################\n",
    "# Extract features from text\n",
    "if matching_use_text:    \n",
    "    qf_base_text = extract_feature_text(model_text, dataset=dataset, split='query')\n",
    "    gf_base_text = extract_feature_text(model_text, dataset=dataset, split='test')\n",
    "    \n",
    "    logging.info(f'qf_base_text = {qf_base_text.shape}')\n",
    "    logging.info(f'gf_base_text = {gf_base_text.shape}')\n",
    "    assert qf_base_text.shape[0] == num_query_base\n",
    "    assert qf_base_text.shape[1] == embedding_size_text\n",
    "    assert gf_base_text.shape[0] == num_gallery_base\n",
    "    assert gf_base_text.shape[1] == embedding_size_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781e99f-d968-42f3-acb6-a7165308eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_pat_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95c25c-43c1-4cc5-ae18-765ffe3f55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_pat_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c03ef0-776e-453c-b368-f95974f7abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_query_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6add9f-0a02-4449-8fb2-6bb56b25bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gallery_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c63a58-c410-4a60-ba40-265e748651cb",
   "metadata": {},
   "source": [
    "# Similarity matrices\n",
    "These dot-product lead to similarities: the higher and closer to 1, the more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7e1c9-6baf-4227-84ba-8cc977ca9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "if matching_use_text and not matching_use_refinements:\n",
    "    logging.info('Case 1: Uses both visual and text, both from base')\n",
    "    q_g_dist_base_visual = np.dot(qf_base, np.transpose(gf_base))\n",
    "    q_g_dist_base_text = np.dot(qf_base_text, np.transpose(gf_base_text))\n",
    "    q_g_dist = (q_g_dist_base_visual + q_g_dist_base_text)/2\n",
    "    #q_g_dist = q_g_dist_base_text\n",
    "    \n",
    "    q_q_dist = np.dot(qf_base, np.transpose(qf_base))\n",
    "    g_g_dist = np.dot(gf_base, np.transpose(gf_base))\n",
    "elif matching_use_refinements and not matching_use_text:\n",
    "    logging.info('Case 2...')\n",
    "\n",
    "    q_g_dist_base = np.dot(qf_base, np.transpose(gf_base))\n",
    "    q_g_dist_A = np.dot(qf_A, np.transpose(gf_A))\n",
    "    q_g_dist_B = np.dot(qf_B, np.transpose(gf_A))\n",
    "    q_g_dist_C = np.dot(qf_C, np.transpose(gf_A))\n",
    "    \n",
    "    # Option 0: just refinement (only makes sense for debugging)\n",
    "    #q_g_dist = q_g_dist_A\n",
    "    \n",
    "    if matching_use_percentile_fusion is None:\n",
    "        logging.info('Case 2.1: Simple average')\n",
    "        q_g_dist = (base_multiplier*q_g_dist_base + 1*q_g_dist_A + 1*q_g_dist_B + 1*q_g_dist_C)/(base_multiplier+number_of_refinements)\n",
    "    else:\n",
    "        logging.info('Case 2.2: query-wise percentile-based fusion')\n",
    "        # https://chatgpt.com/c/6814e8f8-4ca4-800f-bf5a-fd93a8f48278\n",
    "        # 1. for each query, find the p-th percentile of its base scores\n",
    "        base_thresholds = np.percentile(q_g_dist_base, matching_use_percentile_fusion, axis=1)    # shape: (num_query,)\n",
    "        \n",
    "        # 2. build a mask of “hard” pairs where base score is in bottom p%\n",
    "        #    mask[i,j] = True if q_g_dist_base[i,j] <= base_thresholds[i]\n",
    "        hard_mask = q_g_dist_base <= base_thresholds[:, None]                  # (num_query, num_gallery)\n",
    "        \n",
    "        # 3. compute the best refinement score\n",
    "        ref_max    = np.maximum.reduce([q_g_dist_A, q_g_dist_B, q_g_dist_C])    # (num_query, num_gallery)\n",
    "        \n",
    "        # 4. fuse: use the refinement max for “hard” cases, else keep base\n",
    "        q_g_dist = np.where(hard_mask, ref_max, q_g_dist_base)\n",
    "\n",
    "    q_q_dist = np.dot(qf_base, np.transpose(qf_base))\n",
    "    g_g_dist = np.dot(gf_base, np.transpose(gf_base))\n",
    "elif matching_use_refinements and matching_use_text:\n",
    "    logging.info('Case 3: base + refinmentA + text concatenated')\n",
    "    # This is slightly differnet form the rest\n",
    "    # base, refinment A, and text are concatenated (instead of averaged/combined in as the others)\n",
    "    w_img, w_ref, w_txt = 3.0, 2.0, 1.0  # TODO: use the weight from the parameters\n",
    "    \n",
    "    qf_concat = np.concatenate([\n",
    "        w_img  * l2_norm(qf_base),\n",
    "        w_ref  * l2_norm(qf_A),\n",
    "        w_txt  * l2_norm(qf_base_text)\n",
    "    ], axis=1)            # shape: (num_query, D_base + D_ref + D_text)\n",
    "    \n",
    "    gf_concat = np.concatenate([\n",
    "        w_img  * l2_norm(gf_base),\n",
    "        w_ref  * l2_norm(gf_A),\n",
    "        w_txt  * l2_norm(gf_base_text)\n",
    "    ], axis=1)            # shape: (num_gallery, D_total)\n",
    "    \n",
    "    q_g_dist = np.dot(l2_norm(qf_concat), l2_norm(gf_concat.T))\n",
    "    q_q_dist = np.dot(qf_base, np.transpose(qf_base))\n",
    "    g_g_dist = np.dot(gf_base, np.transpose(gf_base))\n",
    "else:\n",
    "    logging.info('Case 4: baseline (non-refined, just good old PAT)')\n",
    "    q_g_dist = np.dot(qf_base, np.transpose(gf_base))\n",
    "    q_q_dist = np.dot(qf_base, np.transpose(qf_base))\n",
    "    g_g_dist = np.dot(gf_base, np.transpose(gf_base))\n",
    "    \n",
    "logging.info(f'Query_Gallery_dist = {q_g_dist.shape}')\n",
    "logging.info(f'Query_Query_dist = {q_q_dist.shape}')\n",
    "logging.info(f'Galery_Galery_dist = {g_g_dist.shape}')\n",
    "\n",
    "assert np.isfinite(q_g_dist).all()\n",
    "assert q_g_dist.mean() > 0.01\n",
    "assert q_g_dist.mean() < 1.1 \n",
    "assert q_g_dist.shape[0] == num_query_base\n",
    "assert q_g_dist.shape[1] == num_gallery_base\n",
    "assert q_q_dist.shape[0] == num_query_base\n",
    "assert q_q_dist.shape[1] == num_query_base\n",
    "assert g_g_dist.shape[0] == num_gallery_base\n",
    "assert g_g_dist.shape[1] == num_gallery_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6aa460-321b-4ab5-9807-3991b30d7fe8",
   "metadata": {},
   "source": [
    "# Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15373332-a53a-4066-8668-653e3c8304a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_rank_dist = re_ranking(q_g_dist, q_q_dist, g_g_dist, k1=re_ranking_k1)\n",
    "indices = np.argsort(re_rank_dist, axis=1)[:, :100]\n",
    "\n",
    "m, n = indices.shape\n",
    "# # logging.info('m: {}  n: {}'.format(m, n))\n",
    "with open(track, 'wb') as f_w:\n",
    "    for i in range(m):\n",
    "        write_line = indices[i] + 1\n",
    "        write_line = ' '.join(map(str, write_line.tolist())) + '\\n'\n",
    "        f_w.write(write_line.encode())\n",
    "\n",
    "\n",
    "lista_nombres = [\"{:06d}.jpg\".format(i) for i in range(1, len(indices) + 1)]\n",
    "output_path = track.split(\".txt\")[0] + \"_submission.csv\"\n",
    "\n",
    "with open(output_path, 'w', newline='') as archivo_csv:\n",
    "    csv_writter = csv.writer(archivo_csv)\n",
    "    csv_writter.writerow(['imageName', 'Corresponding Indexes'])\n",
    "    for numero, track_ in zip(lista_nombres, indices):\n",
    "        track_str = ' '.join(map(str, track_ + 1))\n",
    "        csv_writter.writerow([numero, track_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa40d26-9067-4981-a3de-f28d3c8387a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show some predictions\n",
    "assert os.path.exists(submission_file_name)\n",
    "df = pd.read_csv(submission_file_name)\n",
    "assert df.shape[0] == num_query_base\n",
    "assert df.shape[1] == 2\n",
    "\n",
    "i: int = 0\n",
    "for query_index, query_row in df.iterrows():\n",
    "    logging.info(f\"Top candidates for query {query_row['imageName']}\")\n",
    "    plt.imshow(plt.imread(f\"assets/datasets/{dataset}/image_query/{query_row['imageName']}\"))\n",
    "    plt.title('Query')\n",
    "    plt.show()\n",
    "\n",
    "    for candidate_gallery_id in query_row['Corresponding Indexes'].split()[:10]:\n",
    "        plt.imshow(plt.imread(f\"assets/datasets/{dataset}/image_test/{int(candidate_gallery_id):06d}.jpg\"))\n",
    "        plt.title(f\"Candidate {candidate_gallery_id}\")\n",
    "        plt.show()\n",
    "\n",
    "    i += 1;\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f9a90-9aa7-4617-8a9a-2914b56c2cdf",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320971e8-9d27-4ff3-ad2c-10a813909834",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Backuped everything to {experiment_id}')\n",
    "!cp -r {model_path_b} {model_path_b}_backup_{experiment_id}\n",
    "!cp -r {model_path_embedding_r} {model_path_embedding_r}_backup_{experiment_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cde00-a380-4511-8500-e74548c374d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_message += f\"; commit_hash: {subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('ascii').strip()}\"\n",
    "submission_message += f\"; hyperparameters_train_hash: {calculate_params_hash(hyperparams_train_b)}\"\n",
    "submission_message += f\"; hyperparameters_test_hash: {calculate_params_hash(hyperparams_test_b)}\"\n",
    "submission_message += f\"; matching_use_refinements: {matching_use_refinements}\"\n",
    "submission_message += f\"; matching_use_text: {matching_use_text}\"\n",
    "submission_message += f\"; embeddings_refinements_use_dino: {embeddings_refinements_use_dino}\"\n",
    "submission_message += f\"; matching_use_percentile_fusion: {matching_use_percentile_fusion}\"\n",
    "submission_message += f\"; experiment_ID: {experiment_id}\"\n",
    "logging.info(f'Submitting with message \"{submission_message}\"')\n",
    "\n",
    "# Past results (they also appear in kaggle)\n",
    "# 0.14045 | DSD refinement; commit_hash: 6a355affdb8ee8365325d8dd2aee8df03ea9b715; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; experiment_ID: 1746094978\n",
    "# 0.15746 | DSD refinement; commit_hash: a5d60e0dd908ab82263bf647ec9dc746262a0546; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; experiment_ID: 1746185635\n",
    "# 0.14758 | DSD refinement in gallery; commit_hash: a5d60e0dd908ab82263bf647ec9dc746262a0546; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; experiment_ID: 1746186369\n",
    "# 0.03727 | No refinement just caption; commit_hash: a8a26bde273ea4f3c5b540d784d09e706f5d7d5e; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; experiment_ID: 1746190901\n",
    "# 0.01921 | Refinement featurized with Dino; commit_hash: 1c865c3d6c329cf79c603b0e0f0e0ebf9a6686c8; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; experiment_ID: 1746205635\n",
    "# 0.18669 | Just PAT; commit_hash: 1c865c3d6c329cf79c603b0e0f0e0ebf9a6686c8; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; matching_use_refinements: False; matching_use_text: False; embeddings_refinements_use_dino: False; experiment_ID: 1746207959; commit_hash: 1c865c3d6c329cf79c603b0e0f0e0ebf9a6686c8; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; matching_use_refinements: False; matching_use_text: False; embeddings_refinements_use_dino: False; experiment_ID: 1746207959\n",
    "# 0.14701 | Percentile-matching; commit_hash: 1c865c3d6c329cf79c603b0e0f0e0ebf9a6686c8; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; matching_use_refinements: True; matching_use_text: False; embeddings_refinements_use_dino: False; matching_use_percentile_fusion: None; experiment_ID: 1746355049\n",
    "# 0.07168 | Concatenation base+A+text; commit_hash: 1c865c3d6c329cf79c603b0e0f0e0ebf9a6686c8; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; matching_use_refinements: True; matching_use_text: True; embeddings_refinements_use_dino: False; matching_use_percentile_fusion: None; experiment_ID: 1746356444\n",
    "# Submit the file to the competition\n",
    "# Uncomment only for actual submissions!\n",
    "api.competition_submit(submission_file_name, submission_message, competition_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01a6c6-ffa2-4a90-9b9a-cbe8e364f805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
