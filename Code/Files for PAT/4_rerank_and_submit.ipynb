{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2fa01f-745b-4cc0-accd-1d9c642b27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import re\n",
    "import subprocess\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Literal, List\n",
    "import dotenv\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import glob\n",
    "import hashlib\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from config import cfg\n",
    "from model import make_model\n",
    "from utils.re_ranking import re_ranking\n",
    "from data.build_DG_dataloader import build_reid_test_loader\n",
    "from processor.ori_vit_processor_with_amp import do_inference as do_inf\n",
    "from processor.part_attention_vit_processor import do_inference as do_inf_pat\n",
    "\n",
    "config_train = \"config/UrbanElementsReID_train.yml\"  # \"config/UAM_containers.yml\"\n",
    "#config_test = \"config/UrbanElementsReID_test.yml\"\n",
    "config_test = \"config/UrbanElementsReID_test_reduced.yml\"\n",
    "competition_name = \"urban-reid-challenge\"\n",
    "submission_message = f\"test trained submission\"\n",
    "experiment_id: int = int(time.time())\n",
    "number_of_refinements: int = 3\n",
    "num_gallery = 10  # Fill this manually with the expected number of images in the galary; I don't know how to check automatically\n",
    "base_multiplier = 3 # 1=base have the same value as each refinememnt; base_multiplier=number_of_refinements the base will have the same importance as all the refinements _combined_\n",
    "\n",
    "################ Probably nothing has to be modified from now on ################\n",
    "with open(config_train, 'r') as f:\n",
    "    hyperparams_train = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "with open(config_test, 'r') as f:\n",
    "    hyperparams_test = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "model_path = os.path.join(hyperparams_train['LOG_ROOT'], hyperparams_train['LOG_NAME'])\n",
    "\n",
    "assert dotenv.load_dotenv('../../.env')\n",
    "assert os.getenv('KAGGLE_USERNAME')\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logging.warning(\"Where is your GPU dude?\")\n",
    "\n",
    "max_epoch = int(hyperparams_train['SOLVER']['MAX_EPOCHS'])\n",
    "assert hyperparams_test['TEST']['WEIGHT'].split('/')[-1] == f'part_attention_vit_{max_epoch}.pth', 'not testing with the trained model...'\n",
    "\n",
    "re_ranking_k1: int = 10 if 'reduced' in config_test  else 20\n",
    "\n",
    "submission_file_name = os.path.join(model_path, \"track_submission.csv\")\n",
    "track: str = os.path.join(model_path, \"track.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f51dc32e-29d6-44a1-bfe7-c0f7db0d1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model: nn.Module, dataloaders: DataLoader, subset: Literal['query', 'gallery'], filename_pattern: str = r'\\.') -> np.ndarray:\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: resnet featurizer\n",
    "        dataloaders: for everything\n",
    "        subset: to which suibset should the featurization should be restricted\n",
    "        filename_pattern: matches filename with `re.search`. The default value includes all images\n",
    "    Return:\n",
    "        feature vector shape [<number of images in subset, 768]\n",
    "    '''\n",
    "    assert isinstance(model, nn.Module)\n",
    "    assert isinstance(dataloaders, DataLoader)\n",
    "    assert type(num_query) == int\n",
    "    with torch.no_grad():\n",
    "        features = torch.FloatTensor(0, 768).cuda()\n",
    "        count = 0\n",
    "        img_path = []\n",
    "        for data in dataloaders:\n",
    "            images, _, _, filenames, metadatas = data.values()\n",
    "            #print('filenames 1', filenames)\n",
    "\n",
    "            # Select only the images that belong to the desired subset\n",
    "            subsets: List[Literal['query', 'gallery']] = metadatas['q_or_g']\n",
    "            mask = torch.tensor([s == subset for s in subsets], dtype=torch.bool)\n",
    "            images = images[mask]\n",
    "            filenames = list(np.array(filenames)[mask])\n",
    "            assert len(images) == sum(1 for s in subsets if s == subset)\n",
    "            #print('filenames 2', filenames)\n",
    "\n",
    "            # Select only the images that match the filename pattern\n",
    "            mask = torch.tensor([bool(re.search(filename_pattern, fn)) for fn in filenames], dtype=torch.bool)\n",
    "            images = images[mask]\n",
    "            filenames = list(np.array(filenames)[mask])\n",
    "            assert len(images) == sum(mask), \"Selection count mismatch\"\n",
    "\n",
    "            n, c, h, w = images.size()\n",
    "            if n==0:\n",
    "                continue\n",
    "\n",
    "            count += n\n",
    "            ff = torch.FloatTensor(n, 768).zero_().cuda()  # 2048 is pool5 of resnet\n",
    "            for i in range(2):\n",
    "                input_img = images.cuda()\n",
    "                outputs = model(input_img)\n",
    "                f = outputs.float()\n",
    "                ff = ff + f\n",
    "            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "            ff = ff.div(fnorm.expand_as(ff))\n",
    "            features = torch.cat([features, ff], 0)\n",
    "        assert features.shape[1] == 768\n",
    "        return features.cpu().numpy()\n",
    "\n",
    "def calculate_params_hash(params: Dict[str, Any]) -> str:\n",
    "    stringified = json.dumps({k: str(params[k]) for k in params}, sort_keys=True)\n",
    "    return hashlib.md5(stringified.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16856e47-3859-444b-8738-c7249e1fef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded configuration file config/UrbanElementsReID_test_reduced.yml\n",
      "INFO:root:\n",
      "MODEL:\n",
      "  PRETRAIN_CHOICE: 'imagenet'\n",
      "  #PRETRAIN_PATH: \"../../.cache/torch/hub/checkpoints\" # root of pretrain path\n",
      "  PRETRAIN_PATH: \"assets/models\" \n",
      "  METRIC_LOSS_TYPE: 'triplet'\n",
      "  IF_LABELSMOOTH: 'on'\n",
      "  IF_WITH_CENTER: 'no'\n",
      "  NAME: 'part_attention_vit'\n",
      "  NO_MARGIN: True\n",
      "  DEVICE_ID: ('0')\n",
      "  TRANSFORMER_TYPE: 'vit_base_patch16_224_TransReID'\n",
      "  STRIDE_SIZE: [16, 16]\n",
      "\n",
      "INPUT:\n",
      "  SIZE_TRAIN: [256,128]\n",
      "  SIZE_TEST: [256,128]\n",
      "  REA:\n",
      "    ENABLED: False\n",
      "  PIXEL_MEAN: [0.5, 0.5, 0.5]\n",
      "  PIXEL_STD: [0.5, 0.5, 0.5]\n",
      "  LGT: # Local Grayscale Transfomation\n",
      "    DO_LGT: True\n",
      "    PROB: 0.5\n",
      "\n",
      "DATASETS:\n",
      "  TRAIN: ('UrbanElementsReID',)\n",
      "  TEST: ('UrbanElementsReID_test',)\n",
      "  #ROOT_DIR: ('../../data') # root of datasets\n",
      "  #ROOT_DIR: '/home/jgf/Desktop/rhome/jgf/baselineChallenge/UrbanElementsReID/'\n",
      "  ROOT_DIR: 'assets/datasets/urban-reid-challenge-reduced'\n",
      "\n",
      "\n",
      "DATALOADER:\n",
      "  SAMPLER: 'softmax_triplet'\n",
      "  NUM_INSTANCE: 4\n",
      "  NUM_WORKERS: 8\n",
      "\n",
      "SOLVER:\n",
      "  OPTIMIZER_NAME: 'SGD'\n",
      "  MAX_EPOCHS: 60\n",
      "  BASE_LR: 0.001 # 0.0004 for msmt\n",
      "  IMS_PER_BATCH: 64\n",
      "  WARMUP_METHOD: 'linear'\n",
      "  LARGE_FC_LR: False\n",
      "  CHECKPOINT_PERIOD: 5\n",
      "  LOG_PERIOD: 60\n",
      "  EVAL_PERIOD: 1\n",
      "  WEIGHT_DECAY:  1e-4\n",
      "  WEIGHT_DECAY_BIAS: 1e-4\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  SEED: 1234\n",
      "\n",
      "TEST:\n",
      "  EVAL: True\n",
      "  IMS_PER_BATCH: 128\n",
      "  RE_RANKING: False\n",
      "  WEIGHT: \"assets/models/PAT/part_attention_vit_1.pth\" #test model (epoch number should coincide with the trained epochs)\n",
      "  NECK_FEAT: 'before'\n",
      "  FEAT_NORM: True\n",
      "\n",
      "LOG_ROOT: 'assets/models/' # root of log file\n",
      "TB_LOG_ROOT: './assets/tb_log/'\n",
      "LOG_NAME: 'PAT'\n",
      "INFO:root:Running with config:\n",
      "DATALOADER:\n",
      "  CAMERA_TO_DOMAIN: False\n",
      "  DELETE_REM: False\n",
      "  DROP_LAST: False\n",
      "  INDIVIDUAL: False\n",
      "  NAIVE_WAY: True\n",
      "  NUM_INSTANCE: 4\n",
      "  NUM_WORKERS: 8\n",
      "  SAMPLER: softmax_triplet\n",
      "DATASETS:\n",
      "  COMBINEALL: False\n",
      "  ROOT_DIR: assets/datasets/urban-reid-challenge-reduced\n",
      "  TEST: ('UrbanElementsReID_test',)\n",
      "  TRAIN: ('UrbanElementsReID',)\n",
      "INPUT:\n",
      "  CJ:\n",
      "    BRIGHTNESS: 0.15\n",
      "    CONTRAST: 0.15\n",
      "    ENABLED: False\n",
      "    HUE: 0.1\n",
      "    PROB: 1.0\n",
      "    SATURATION: 0.1\n",
      "  DO_AUGMIX: False\n",
      "  DO_AUTOAUG: False\n",
      "  DO_FLIP: True\n",
      "  DO_PAD: True\n",
      "  FLIP_PROB: 0.5\n",
      "  LGT:\n",
      "    DO_LGT: True\n",
      "    PROB: 0.5\n",
      "  PADDING: 10\n",
      "  PADDING_MODE: constant\n",
      "  PIXEL_MEAN: [0.5, 0.5, 0.5]\n",
      "  PIXEL_STD: [0.5, 0.5, 0.5]\n",
      "  REA:\n",
      "    ENABLED: False\n",
      "    MEAN: [123.675, 116.28, 103.53]\n",
      "    PROB: 0.5\n",
      "  RPT:\n",
      "    ENABLED: False\n",
      "    PROB: 0.5\n",
      "  SIZE_TEST: [256, 128]\n",
      "  SIZE_TRAIN: [256, 128]\n",
      "LOG_NAME: PAT\n",
      "LOG_ROOT: assets/models/\n",
      "MODEL:\n",
      "  ATT_DROP_RATE: 0.0\n",
      "  CLUSTER_K: 10\n",
      "  COS_LAYER: False\n",
      "  DEVICE: cuda\n",
      "  DEVICE_ID: 0\n",
      "  DIST_TRAIN: False\n",
      "  DROP_OUT: 0.0\n",
      "  DROP_PATH: 0.1\n",
      "  FREEZE_PATCH_EMBED: True\n",
      "  ID_LOSS_TYPE: softmax\n",
      "  ID_LOSS_WEIGHT: 1.0\n",
      "  IF_LABELSMOOTH: on\n",
      "  IF_WITH_CENTER: no\n",
      "  LAST_STRIDE: 1\n",
      "  METRIC_LOSS_TYPE: triplet\n",
      "  NAME: part_attention_vit\n",
      "  NECK: bnneck\n",
      "  NO_MARGIN: True\n",
      "  PATCH_EMBED_TYPE: \n",
      "  PC_LOSS: True\n",
      "  PC_LR: 1.0\n",
      "  PC_SCALE: 0.02\n",
      "  PRETRAIN_CHOICE: imagenet\n",
      "  PRETRAIN_PATH: assets/models\n",
      "  SOFT_LABEL: True\n",
      "  SOFT_LAMBDA: 0.5\n",
      "  SOFT_WEIGHT: 0.5\n",
      "  STRIDE_SIZE: [16, 16]\n",
      "  TRANSFORMER_TYPE: vit_base_patch16_224_TransReID\n",
      "  TRIPLET_LOSS_WEIGHT: 1.0\n",
      "SOLVER:\n",
      "  BASE_LR: 0.001\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  CENTER_LOSS_WEIGHT: 0.0005\n",
      "  CENTER_LR: 0.5\n",
      "  CHECKPOINT_PERIOD: 5\n",
      "  COSINE_MARGIN: 0.5\n",
      "  COSINE_SCALE: 30\n",
      "  EVAL_PERIOD: 1\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 64\n",
      "  LARGE_FC_LR: False\n",
      "  LOG_PERIOD: 60\n",
      "  MARGIN: 0.3\n",
      "  MAX_EPOCHS: 60\n",
      "  MOMENTUM: 0.9\n",
      "  OPTIMIZER_NAME: SGD\n",
      "  SEED: 1234\n",
      "  STEPS: (40, 70)\n",
      "  WARMUP_EPOCHS: 5\n",
      "  WARMUP_FACTOR: 0.01\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: 0.0001\n",
      "TB_LOG_ROOT: ./assets/tb_log/\n",
      "TEST:\n",
      "  DIST_MAT: dist_mat.npy\n",
      "  EVAL: True\n",
      "  FEAT_NORM: True\n",
      "  IMS_PER_BATCH: 128\n",
      "  NECK_FEAT: before\n",
      "  RE_RANKING: False\n",
      "  WEIGHT: assets/models/PAT/part_attention_vit_1.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Transformer_type: part token vit as a backbone\n",
      "using stride: [16, 16], and patch number is num_y16 * num_x8\n",
      "using drop_out rate is : 0.0\n",
      "using attn_drop_out rate is : 0.0\n",
      "using drop_path rate is : 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "INFO:PAT.train:Number of parameter: 86.52M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized position embedding from size:torch.Size([1, 197, 768]) to size: torch.Size([1, 132, 768]) with height:16 width: 8\n",
      "Load 153 / 155 layers.\n",
      "Loading pretrained ImageNet model......from assets/models/jx_vit_base_p16_224-80ecf9dd.pth\n",
      "===========building our part attention vit===========\n",
      "Loading trained model from assets/models/PAT/part_attention_vit_1.pth\n"
     ]
    }
   ],
   "source": [
    "cfg.merge_from_file(config_test)\n",
    "cfg.freeze()\n",
    "\n",
    "output_dir = os.path.join(cfg.LOG_ROOT, cfg.LOG_NAME)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "logging.info(\"Loaded configuration file {}\".format(config_test))\n",
    "with open(config_test, 'r') as cf:\n",
    "    config_str = \"\\n\" + cf.read()\n",
    "    logging.info(config_str)\n",
    "logging.info(\"Running with config:\\n{}\".format(cfg))\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cfg.MODEL.DEVICE_ID\n",
    "\n",
    "model = make_model(cfg, cfg.MODEL.NAME, 0,0,0)\n",
    "model.load_param(cfg.TEST.WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf3d36f-0c6f-42d1-b69f-e84b19a35848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:>>>>>>>>>>>>>>>>>>>>>>> UrbanElementsReID_test\n",
      "INFO:root:Dataset and root: UrbanElementsReID_test, and assets/datasets/urban-reid-challenge-reduced\n",
      "INFO:PAT:=> Loaded UrbanElementsReID_test\n",
      "INFO:PAT:  ----------------------------------------\n",
      "INFO:PAT:  subset   | # ids | # images | # cameras\n",
      "INFO:PAT:  ----------------------------------------\n",
      "INFO:PAT:  query    |     1 |       12 |         1\n",
      "INFO:PAT:  gallery  |     1 |       10 |         3\n",
      "INFO:PAT:  ----------------------------------------\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "INFO:PAT.test:Enter inferencing\n",
      "WARNING:root:Number of gallery samples is quite small, got 10\n",
      "INFO:PAT.test:Validation Results \n",
      "INFO:PAT.test:mAP: 100.0%\n",
      "INFO:PAT.test:CMC curve, Rank-1  :100.0%\n",
      "INFO:PAT.test:CMC curve, Rank-5  :100.0%\n",
      "INFO:PAT.test:CMC curve, Rank-10 :100.0%\n",
      "INFO:PAT.test:total inference time: 1.31\n"
     ]
    }
   ],
   "source": [
    "for testname in cfg.DATASETS.TEST:\n",
    "    logging.info(f'>>>>>>>>>>>>>>>>>>>>>>> {testname}')\n",
    "    val_loader, num_query = build_reid_test_loader(cfg, testname)\n",
    "    if cfg.MODEL.NAME == 'part_attention_vit':\n",
    "        do_inf_pat(cfg, model, val_loader, num_query)\n",
    "    else:\n",
    "        do_inf(cfg, model, val_loader, num_query)\n",
    "\n",
    "    #logging.info(type(model))\n",
    "    #logging.info(type(val_loader))\n",
    "    #logging.info(type(num_query))\n",
    "num_query_base = int(num_query/(1+number_of_refinements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2e0fb-5a13-44d6-905e-d0508e0355bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8accee9-7dc2-4cde-a3aa-d374d3a5cf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad741c1-64a7-44c0-9c58-e05d17e782c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04f900-46d3-4409-9ca9-c495061a6e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20610d3b-dbb8-47b6-8fe1-ba9288a013da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:qf_base = (3, 768)\n",
      "INFO:root:qf_A = (3, 768)\n",
      "INFO:root:qf_B = (3, 768)\n",
      "INFO:root:qf_C = (3, 768)\n"
     ]
    }
   ],
   "source": [
    "#qf_base = extract_feature(model, val_loader, subset=r'query')\n",
    "qf_base = extract_feature(model, val_loader, subset='query', filename_pattern = r'^(?!.*refinement).*$')\n",
    "qf_A = extract_feature(model, val_loader, subset='query', filename_pattern = r'_refinement_A\\.')\n",
    "qf_B = extract_feature(model, val_loader, subset='query', filename_pattern = r'_refinement_B\\.')\n",
    "qf_C = extract_feature(model, val_loader, subset='query', filename_pattern = r'_refinement_C\\.')\n",
    "gf = extract_feature(model, val_loader, subset='gallery')\n",
    "\n",
    "logging.info(f'qf_base = {qf_base.shape}')\n",
    "logging.info(f'qf_A = {qf_A.shape}')\n",
    "logging.info(f'qf_B = {qf_B.shape}')\n",
    "logging.info(f'qf_C = {qf_C.shape}')\n",
    "\n",
    "assert qf_base.shape[0] == num_query_base\n",
    "assert qf_A.shape[0] == num_query_base\n",
    "assert qf_B.shape[0] == num_query_base\n",
    "assert qf_B.shape[0] == num_query_base\n",
    "assert gf.shape[0] == num_gallery\n",
    "#np.save(\"./qf.npy\", qf_base)\n",
    "#np.save(\"./gf.npy\", gf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27a9d-fffe-467d-bfef-ed3360328976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c7e1c9-6baf-4227-84ba-8cc977ca9675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Query_Gallery_dist = (3, 10)\n",
      "INFO:root:Query_Query_dist = (3, 3)\n",
      "INFO:root:Galery_Galery_dist = (10, 10)\n"
     ]
    }
   ],
   "source": [
    "#q_g_dist = np.dot(qf_base, np.transpose(gf)) # TODO: This one will have to be calculated 3 times, and averaged out\n",
    "q_g_dist_base = np.dot(qf_base, np.transpose(gf))\n",
    "q_g_dist_A = np.dot(qf_A, np.transpose(gf))\n",
    "q_g_dist_B = np.dot(qf_B, np.transpose(gf))\n",
    "q_g_dist_C = np.dot(qf_C, np.transpose(gf))\n",
    "q_g_dist = (base_multiplier*q_g_dist_base + 1*q_g_dist_A + 1*q_g_dist_B + 1*q_g_dist_C)/(base_multiplier+number_of_refinements)\n",
    "\n",
    "q_q_dist = np.dot(qf_base, np.transpose(qf_base))\n",
    "g_g_dist = np.dot(gf, np.transpose(gf))\n",
    "\n",
    "logging.info(f'Query_Gallery_dist = {q_g_dist.shape}')\n",
    "logging.info(f'Query_Query_dist = {q_q_dist.shape}')\n",
    "logging.info(f'Galery_Galery_dist = {g_g_dist.shape}')\n",
    "\n",
    "assert q_g_dist.shape[0] == num_query_base\n",
    "assert q_g_dist.shape[1] == num_gallery\n",
    "assert q_q_dist.shape[0] == num_query_base\n",
    "assert q_q_dist.shape[1] == num_query_base\n",
    "assert g_g_dist.shape[0] == num_gallery\n",
    "assert g_g_dist.shape[1] == num_gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfb3cd-da44-4400-9384-52a09dbca528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dd5a6-61bf-45c7-b965-9f5417a1610f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033ce1e-1722-4d68-ac70-5a33b9115a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15373332-a53a-4066-8668-653e3c8304a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_rank_dist = re_ranking(q_g_dist, q_q_dist, g_g_dist, k1=re_ranking_k1)\n",
    "indices = np.argsort(re_rank_dist, axis=1)[:, :100]\n",
    "\n",
    "m, n = indices.shape\n",
    "# # logging.info('m: {}  n: {}'.format(m, n))\n",
    "with open(track, 'wb') as f_w:\n",
    "    for i in range(m):\n",
    "        write_line = indices[i] + 1\n",
    "        write_line = ' '.join(map(str, write_line.tolist())) + '\\n'\n",
    "        f_w.write(write_line.encode())\n",
    "\n",
    "\n",
    "lista_nombres = [\"{:06d}.jpg\".format(i) for i in range(1, len(indices) + 1)]\n",
    "output_path = track.split(\".txt\")[0] + \"_submission.csv\"\n",
    "\n",
    "with open(output_path, 'w', newline='') as archivo_csv:\n",
    "    csv_writter = csv.writer(archivo_csv)\n",
    "    csv_writter.writerow(['imageName', 'Corresponding Indexes'])\n",
    "    for numero, track in zip(lista_nombres, indices):\n",
    "        track_str = ' '.join(map(str, track + 1))\n",
    "        csv_writter.writerow([numero, track_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611889a-cc83-4053-976b-9211fbebfc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320971e8-9d27-4ff3-ad2c-10a813909834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Backuped everything to 1746007678\n"
     ]
    }
   ],
   "source": [
    "#!python update.py --config_file {config_test} --track {os.path.join(model_path, \"track.txt\")}\n",
    "\n",
    "assert os.path.exists(submission_file_name)\n",
    "logging.info(f'Backuped everything to {experiment_id}')\n",
    "\n",
    "!cp -r {model_path} {model_path}_backup_{experiment_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6cde00-a380-4511-8500-e74548c374d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Submitting with message \"test trained submission; commit_hash: f10c2a9ccae085b8d1c9ff3768d50d6c141b1993; hyperparameters_train_hash: d3eff5185fae9d77e36e071d6716704d; hyperparameters_test_hash: fc1a7d2b1f68ce959365766eeb6e8186; experiment_ID: 1746007678\"\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(submission_file_name)\n",
    "df = pd.read_csv(submission_file_name)\n",
    "assert df.shape[1] == 2\n",
    "assert df.shape[0] >= 3\n",
    "\n",
    "submission_message += f\"; commit_hash: {subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('ascii').strip()}\"\n",
    "submission_message += f\"; hyperparameters_train_hash: {calculate_params_hash(hyperparams_train)}\"\n",
    "submission_message += f\"; hyperparameters_test_hash: {calculate_params_hash(hyperparams_test)}\"\n",
    "submission_message += f\"; experiment_ID: {experiment_id}\"\n",
    "logging.info(f'Submitting with message \"{submission_message}\"')\n",
    "\n",
    "# Submit the file to the competition\n",
    "# Uncomment only for actual submissions!\n",
    "#api.competition_submit(submission_file_name, submission_message, competition_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3051c2a-6864-455b-8c01-044169bda49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
