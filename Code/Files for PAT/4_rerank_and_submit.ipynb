{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a2fa01f-745b-4cc0-accd-1d9c642b27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import re\n",
    "import subprocess\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Literal, List\n",
    "import dotenv\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import glob\n",
    "import hashlib\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from config import cfg\n",
    "from model import make_model\n",
    "from utils.re_ranking import re_ranking\n",
    "from data.build_DG_dataloader import build_reid_test_loader\n",
    "from processor.ori_vit_processor_with_amp import do_inference as do_inf\n",
    "from processor.part_attention_vit_processor import do_inference as do_inf_pat\n",
    "\n",
    "config_train = \"config/UrbanElementsReID_train.yml\"  # \"config/UAM_containers.yml\"\n",
    "config_test = \"config/UrbanElementsReID_test.yml\"\n",
    "#config_test = \"config/UrbanElementsReID_test_reduced.yml\"\n",
    "competition_name = \"urban-reid-challenge\"\n",
    "submission_message = f\"DSD refinement\"\n",
    "experiment_id: int = int(time.time())\n",
    "number_of_refinements: int = 3\n",
    "base_multiplier = 1 # 1=base have the same value as each refinememnt; base_multiplier=number_of_refinements the base will have the same importance as all the refinements _combined_\n",
    "\n",
    "################ Probably nothing has to be modified from now on ################\n",
    "with open(config_train, 'r') as f:\n",
    "    hyperparams_train = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "with open(config_test, 'r') as f:\n",
    "    hyperparams_test = yaml.load(f, Loader=yaml.BaseLoader)\n",
    "model_path = os.path.join(hyperparams_train['LOG_ROOT'], hyperparams_train['LOG_NAME'])\n",
    "\n",
    "assert dotenv.load_dotenv('../../.env')\n",
    "assert os.getenv('KAGGLE_USERNAME')\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logging.warning(\"Where is your GPU dude?\")\n",
    "\n",
    "max_epoch = int(hyperparams_train['SOLVER']['MAX_EPOCHS'])\n",
    "assert hyperparams_test['TEST']['WEIGHT'].split('/')[-1] == f'part_attention_vit_{max_epoch}.pth', 'not testing with the trained model...'\n",
    "\n",
    "re_ranking_k1: int = 10 if 'reduced' in config_test  else 20\n",
    "num_gallery = sum(1 for f in Path(hyperparams_test['DATASETS']['ROOT_DIR']+'/image_test').iterdir() if f.is_file())  # Expected number of images in the galary\n",
    "submission_file_name = os.path.join(model_path, \"track_submission.csv\")\n",
    "track: str = os.path.join(model_path, \"track.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d375e1f5-98d5-4c7e-a443-152415a72e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assets/datasets/urban-reid-challenge'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams_test['DATASETS']['ROOT_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd9e71b7-e1de-4863-8042-356bfaf1ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: zip: not found\n"
     ]
    }
   ],
   "source": [
    "!zip -r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a00e6d55-e199-4879-83a1-fa115ddd6bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a35673a7-412b-47b1-aeb4-23aa41a300ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  unzip\n",
      "The following NEW packages will be installed:\n",
      "  unzip zip\n",
      "0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 350 kB of archives.\n",
      "After this operation, 930 kB of additional disk space will be used.\n",
      "Err:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 unzip amd64 6.0-26ubuntu3.2\n",
      "  Could not open file /var/cache/apt/archives/partial/unzip_6.0-26ubuntu3.2_amd64.deb - open (30: Read-only file system) [IP: 91.189.91.81 80]\n",
      "Err:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 zip amd64 3.0-12build2\n",
      "  Could not open file /var/cache/apt/archives/partial/zip_3.0-12build2_amd64.deb - open (30: Read-only file system) [IP: 185.125.190.82 80]\n",
      "\u001b[1;33mW: \u001b[0mNot using locking for read only lock file /var/lib/dpkg/lock-frontend\u001b[0m\n",
      "\u001b[1;33mW: \u001b[0mNot using locking for read only lock file /var/lib/dpkg/lock\u001b[0m\n",
      "\u001b[1;33mW: \u001b[0mNot using locking for read only lock file /var/cache/apt/archives/lock\u001b[0m\n",
      "\u001b[1;33mW: \u001b[0mProblem unlinking the file /var/cache/apt/archives/partial/unzip_6.0-26ubuntu3.2_amd64.deb - PrepareFiles (30: Read-only file system)\u001b[0m\n",
      "\u001b[1;33mW: \u001b[0mProblem unlinking the file /var/cache/apt/archives/partial/zip_3.0-12build2_amd64.deb - PrepareFiles (30: Read-only file system)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mFailed to fetch http://archive.ubuntu.com/ubuntu/pool/main/u/unzip/unzip_6.0-26ubuntu3.2_amd64.deb  Could not open file /var/cache/apt/archives/partial/unzip_6.0-26ubuntu3.2_amd64.deb - open (30: Read-only file system) [IP: 91.189.91.81 80]\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mFailed to fetch http://archive.ubuntu.com/ubuntu/pool/main/z/zip/zip_3.0-12build2_amd64.deb  Could not open file /var/cache/apt/archives/partial/zip_3.0-12build2_amd64.deb - open (30: Read-only file system) [IP: 185.125.190.82 80]\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to fetch some archives, maybe run apt-get update or try with --fix-missing?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt install zip -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f51dc32e-29d6-44a1-bfe7-c0f7db0d1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model: nn.Module, dataloaders: DataLoader, subset: Literal['query', 'gallery'], filename_pattern: str = r'\\.') -> np.ndarray:\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: resnet featurizer\n",
    "        dataloaders: for everything\n",
    "        subset: to which suibset should the featurization should be restricted\n",
    "        filename_pattern: matches filename with `re.search`. The default value includes all images\n",
    "    Return:\n",
    "        feature vector shape [<number of images in subset, 768]\n",
    "    '''\n",
    "    assert isinstance(model, nn.Module)\n",
    "    assert isinstance(dataloaders, DataLoader)\n",
    "    assert type(num_query) == int\n",
    "    with torch.no_grad():\n",
    "        features = torch.FloatTensor(0, 768).cuda()\n",
    "        count = 0\n",
    "        img_path = []\n",
    "        for data in dataloaders:\n",
    "            images, _, _, filenames, metadatas = data.values()\n",
    "            #print('filenames 1', filenames)\n",
    "\n",
    "            # Select only the images that belong to the desired subset\n",
    "            subsets: List[Literal['query', 'gallery']] = metadatas['q_or_g']\n",
    "            mask = torch.tensor([s == subset for s in subsets], dtype=torch.bool)\n",
    "            images = images[mask]\n",
    "            filenames = list(np.array(filenames)[mask])\n",
    "            assert len(images) == sum(1 for s in subsets if s == subset)\n",
    "            #print('filenames 2', filenames)\n",
    "\n",
    "            # Select only the images that match the filename pattern\n",
    "            mask = torch.tensor([bool(re.search(filename_pattern, fn)) for fn in filenames], dtype=torch.bool)\n",
    "            images = images[mask]\n",
    "            filenames = list(np.array(filenames)[mask])\n",
    "            assert len(images) == sum(mask), \"Selection count mismatch\"\n",
    "\n",
    "            n, c, h, w = images.size()\n",
    "            if n==0:\n",
    "                continue\n",
    "\n",
    "            count += n\n",
    "            ff = torch.FloatTensor(n, 768).zero_().cuda()  # 2048 is pool5 of resnet\n",
    "            for i in range(2):\n",
    "                input_img = images.cuda()\n",
    "                outputs = model(input_img)\n",
    "                f = outputs.float()\n",
    "                ff = ff + f\n",
    "            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "            ff = ff.div(fnorm.expand_as(ff))\n",
    "            features = torch.cat([features, ff], 0)\n",
    "        assert features.shape[1] == 768\n",
    "        return features.cpu().numpy()\n",
    "\n",
    "def calculate_params_hash(params: Dict[str, Any]) -> str:\n",
    "    stringified = json.dumps({k: str(params[k]) for k in params}, sort_keys=True)\n",
    "    return hashlib.md5(stringified.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16856e47-3859-444b-8738-c7249e1fef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded configuration file config/UrbanElementsReID_test.yml\n",
      "INFO:root:\n",
      "MODEL:\n",
      "  PRETRAIN_CHOICE: 'imagenet'\n",
      "  #PRETRAIN_PATH: \"../../.cache/torch/hub/checkpoints\" # root of pretrain path\n",
      "  PRETRAIN_PATH: \"assets/models\" \n",
      "  METRIC_LOSS_TYPE: 'triplet'\n",
      "  IF_LABELSMOOTH: 'on'\n",
      "  IF_WITH_CENTER: 'no'\n",
      "  NAME: 'part_attention_vit'\n",
      "  NO_MARGIN: True\n",
      "  DEVICE_ID: ('0')\n",
      "  TRANSFORMER_TYPE: 'vit_base_patch16_224_TransReID'\n",
      "  STRIDE_SIZE: [16, 16]\n",
      "\n",
      "INPUT:\n",
      "  SIZE_TRAIN: [256,128]\n",
      "  SIZE_TEST: [256,128]\n",
      "  REA:\n",
      "    ENABLED: False\n",
      "  PIXEL_MEAN: [0.5, 0.5, 0.5]\n",
      "  PIXEL_STD: [0.5, 0.5, 0.5]\n",
      "  LGT: # Local Grayscale Transfomation\n",
      "    DO_LGT: True\n",
      "    PROB: 0.5\n",
      "\n",
      "DATASETS:\n",
      "  TRAIN: ('UrbanElementsReID',)\n",
      "  TEST: ('UrbanElementsReID_test',)\n",
      "  #ROOT_DIR: ('../../data') # root of datasets\n",
      "  #ROOT_DIR: '/home/jgf/Desktop/rhome/jgf/baselineChallenge/UrbanElementsReID/'\n",
      "  ROOT_DIR: 'assets/datasets/urban-reid-challenge'\n",
      "\n",
      "\n",
      "DATALOADER:\n",
      "  SAMPLER: 'softmax_triplet'\n",
      "  NUM_INSTANCE: 4\n",
      "  NUM_WORKERS: 8\n",
      "\n",
      "SOLVER:\n",
      "  OPTIMIZER_NAME: 'SGD'\n",
      "  MAX_EPOCHS: 60\n",
      "  BASE_LR: 0.001 # 0.0004 for msmt\n",
      "  IMS_PER_BATCH: 64\n",
      "  WARMUP_METHOD: 'linear'\n",
      "  LARGE_FC_LR: False\n",
      "  CHECKPOINT_PERIOD: 5\n",
      "  LOG_PERIOD: 60\n",
      "  EVAL_PERIOD: 1\n",
      "  WEIGHT_DECAY:  1e-4\n",
      "  WEIGHT_DECAY_BIAS: 1e-4\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  SEED: 1234\n",
      "\n",
      "TEST:\n",
      "  EVAL: True\n",
      "  IMS_PER_BATCH: 128\n",
      "  RE_RANKING: False\n",
      "  WEIGHT: \"assets/models/PAT/part_attention_vit_60.pth\" #test model (epoch number should coincide with the trained epochs)\n",
      "  NECK_FEAT: 'before'\n",
      "  FEAT_NORM: True\n",
      "\n",
      "LOG_ROOT: 'assets/models/' # root of log file\n",
      "TB_LOG_ROOT: './assets/tb_log/'\n",
      "LOG_NAME: 'PAT'\n",
      "INFO:root:Running with config:\n",
      "DATALOADER:\n",
      "  CAMERA_TO_DOMAIN: False\n",
      "  DELETE_REM: False\n",
      "  DROP_LAST: False\n",
      "  INDIVIDUAL: False\n",
      "  NAIVE_WAY: True\n",
      "  NUM_INSTANCE: 4\n",
      "  NUM_WORKERS: 8\n",
      "  SAMPLER: softmax_triplet\n",
      "DATASETS:\n",
      "  COMBINEALL: False\n",
      "  ROOT_DIR: assets/datasets/urban-reid-challenge\n",
      "  TEST: ('UrbanElementsReID_test',)\n",
      "  TRAIN: ('UrbanElementsReID',)\n",
      "INPUT:\n",
      "  CJ:\n",
      "    BRIGHTNESS: 0.15\n",
      "    CONTRAST: 0.15\n",
      "    ENABLED: False\n",
      "    HUE: 0.1\n",
      "    PROB: 1.0\n",
      "    SATURATION: 0.1\n",
      "  DO_AUGMIX: False\n",
      "  DO_AUTOAUG: False\n",
      "  DO_FLIP: True\n",
      "  DO_PAD: True\n",
      "  FLIP_PROB: 0.5\n",
      "  LGT:\n",
      "    DO_LGT: True\n",
      "    PROB: 0.5\n",
      "  PADDING: 10\n",
      "  PADDING_MODE: constant\n",
      "  PIXEL_MEAN: [0.5, 0.5, 0.5]\n",
      "  PIXEL_STD: [0.5, 0.5, 0.5]\n",
      "  REA:\n",
      "    ENABLED: False\n",
      "    MEAN: [123.675, 116.28, 103.53]\n",
      "    PROB: 0.5\n",
      "  RPT:\n",
      "    ENABLED: False\n",
      "    PROB: 0.5\n",
      "  SIZE_TEST: [256, 128]\n",
      "  SIZE_TRAIN: [256, 128]\n",
      "LOG_NAME: PAT\n",
      "LOG_ROOT: assets/models/\n",
      "MODEL:\n",
      "  ATT_DROP_RATE: 0.0\n",
      "  CLUSTER_K: 10\n",
      "  COS_LAYER: False\n",
      "  DEVICE: cuda\n",
      "  DEVICE_ID: 0\n",
      "  DIST_TRAIN: False\n",
      "  DROP_OUT: 0.0\n",
      "  DROP_PATH: 0.1\n",
      "  FREEZE_PATCH_EMBED: True\n",
      "  ID_LOSS_TYPE: softmax\n",
      "  ID_LOSS_WEIGHT: 1.0\n",
      "  IF_LABELSMOOTH: on\n",
      "  IF_WITH_CENTER: no\n",
      "  LAST_STRIDE: 1\n",
      "  METRIC_LOSS_TYPE: triplet\n",
      "  NAME: part_attention_vit\n",
      "  NECK: bnneck\n",
      "  NO_MARGIN: True\n",
      "  PATCH_EMBED_TYPE: \n",
      "  PC_LOSS: True\n",
      "  PC_LR: 1.0\n",
      "  PC_SCALE: 0.02\n",
      "  PRETRAIN_CHOICE: imagenet\n",
      "  PRETRAIN_PATH: assets/models\n",
      "  SOFT_LABEL: True\n",
      "  SOFT_LAMBDA: 0.5\n",
      "  SOFT_WEIGHT: 0.5\n",
      "  STRIDE_SIZE: [16, 16]\n",
      "  TRANSFORMER_TYPE: vit_base_patch16_224_TransReID\n",
      "  TRIPLET_LOSS_WEIGHT: 1.0\n",
      "SOLVER:\n",
      "  BASE_LR: 0.001\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  CENTER_LOSS_WEIGHT: 0.0005\n",
      "  CENTER_LR: 0.5\n",
      "  CHECKPOINT_PERIOD: 5\n",
      "  COSINE_MARGIN: 0.5\n",
      "  COSINE_SCALE: 30\n",
      "  EVAL_PERIOD: 1\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 64\n",
      "  LARGE_FC_LR: False\n",
      "  LOG_PERIOD: 60\n",
      "  MARGIN: 0.3\n",
      "  MAX_EPOCHS: 60\n",
      "  MOMENTUM: 0.9\n",
      "  OPTIMIZER_NAME: SGD\n",
      "  SEED: 1234\n",
      "  STEPS: (40, 70)\n",
      "  WARMUP_EPOCHS: 5\n",
      "  WARMUP_FACTOR: 0.01\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: 0.0001\n",
      "TB_LOG_ROOT: ./assets/tb_log/\n",
      "TEST:\n",
      "  DIST_MAT: dist_mat.npy\n",
      "  EVAL: True\n",
      "  FEAT_NORM: True\n",
      "  IMS_PER_BATCH: 128\n",
      "  NECK_FEAT: before\n",
      "  RE_RANKING: False\n",
      "  WEIGHT: assets/models/PAT/part_attention_vit_60.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Transformer_type: part token vit as a backbone\n",
      "using stride: [16, 16], and patch number is num_y16 * num_x8\n",
      "using drop_out rate is : 0.0\n",
      "using attn_drop_out rate is : 0.0\n",
      "using drop_path rate is : 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PAT.train:Number of parameter: 86.52M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized position embedding from size:torch.Size([1, 197, 768]) to size: torch.Size([1, 132, 768]) with height:16 width: 8\n",
      "Load 153 / 155 layers.\n",
      "Loading pretrained ImageNet model......from assets/models/jx_vit_base_p16_224-80ecf9dd.pth\n",
      "===========building our part attention vit===========\n",
      "Loading trained model from assets/models/PAT/part_attention_vit_60.pth\n"
     ]
    }
   ],
   "source": [
    "cfg.merge_from_file(config_test)\n",
    "cfg.freeze()\n",
    "\n",
    "output_dir = os.path.join(cfg.LOG_ROOT, cfg.LOG_NAME)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "logging.info(\"Loaded configuration file {}\".format(config_test))\n",
    "with open(config_test, 'r') as cf:\n",
    "    config_str = \"\\n\" + cf.read()\n",
    "    logging.info(config_str)\n",
    "logging.info(\"Running with config:\\n{}\".format(cfg))\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cfg.MODEL.DEVICE_ID\n",
    "\n",
    "model = make_model(cfg, cfg.MODEL.NAME, 0,0,0)\n",
    "model.load_param(cfg.TEST.WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cf3d36f-0c6f-42d1-b69f-e84b19a35848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:>>>>>>>>>>>>>>>>>>>>>>> UrbanElementsReID_test\n",
      "INFO:root:Dataset and root: UrbanElementsReID_test, and assets/datasets/urban-reid-challenge\n",
      "INFO:PAT:=> Loaded UrbanElementsReID_test\n",
      "INFO:PAT:  ----------------------------------------\n",
      "INFO:PAT:  subset   | # ids | # images | # cameras\n",
      "INFO:PAT:  ----------------------------------------\n",
      "INFO:PAT:  query    |     1 |     1384 |         1\n",
      "INFO:PAT:  gallery  |     1 |     1008 |         3\n",
      "INFO:PAT:  ----------------------------------------\n",
      "INFO:PAT.test:Enter inferencing\n",
      "INFO:PAT.test:Validation Results \n",
      "INFO:PAT.test:mAP: 100.0%\n",
      "INFO:PAT.test:CMC curve, Rank-1  :100.0%\n",
      "INFO:PAT.test:CMC curve, Rank-5  :100.0%\n",
      "INFO:PAT.test:CMC curve, Rank-10 :100.0%\n",
      "INFO:PAT.test:total inference time: 12.05\n"
     ]
    }
   ],
   "source": [
    "for testname in cfg.DATASETS.TEST:\n",
    "    logging.info(f'>>>>>>>>>>>>>>>>>>>>>>> {testname}')\n",
    "    val_loader, num_query = build_reid_test_loader(cfg, testname)\n",
    "    if cfg.MODEL.NAME == 'part_attention_vit':\n",
    "        do_inf_pat(cfg, model, val_loader, num_query)\n",
    "    else:\n",
    "        do_inf(cfg, model, val_loader, num_query)\n",
    "\n",
    "    #logging.info(type(model))\n",
    "    #logging.info(type(val_loader))\n",
    "    #logging.info(type(num_query))\n",
    "num_query_base = int(num_query/(1+number_of_refinements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04f900-46d3-4409-9ca9-c495061a6e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20610d3b-dbb8-47b6-8fe1-ba9288a013da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:qf_base = (346, 768)\n",
      "INFO:root:qf_A = (346, 768)\n",
      "INFO:root:qf_B = (346, 768)\n",
      "INFO:root:qf_C = (346, 768)\n"
     ]
    }
   ],
   "source": [
    "#qf_base = extract_feature(model, val_loader, subset=r'query')\n",
    "qf_base = extract_feature(model, val_loader, subset='query', filename_pattern = r'^(?!.*refinement).*$')\n",
    "qf_A = extract_feature(model, val_loader, subset='query', filename_pattern = r'_refinement_A\\.')\n",
    "qf_B = extract_feature(model, val_loader, subset='query', filename_pattern = r'_refinement_B\\.')\n",
    "qf_C = extract_feature(model, val_loader, subset='query', filename_pattern = r'_refinement_C\\.')\n",
    "gf = extract_feature(model, val_loader, subset='gallery')\n",
    "\n",
    "logging.info(f'qf_base = {qf_base.shape}')\n",
    "logging.info(f'qf_A = {qf_A.shape}')\n",
    "logging.info(f'qf_B = {qf_B.shape}')\n",
    "logging.info(f'qf_C = {qf_C.shape}')\n",
    "\n",
    "assert qf_base.shape[0] == num_query_base\n",
    "assert qf_A.shape[0] == num_query_base\n",
    "assert qf_B.shape[0] == num_query_base\n",
    "assert qf_B.shape[0] == num_query_base\n",
    "assert gf.shape[0] == num_gallery\n",
    "#np.save(\"./qf.npy\", qf_base)\n",
    "#np.save(\"./gf.npy\", gf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27a9d-fffe-467d-bfef-ed3360328976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32c7e1c9-6baf-4227-84ba-8cc977ca9675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Query_Gallery_dist = (346, 1008)\n",
      "INFO:root:Query_Query_dist = (346, 346)\n",
      "INFO:root:Galery_Galery_dist = (1008, 1008)\n"
     ]
    }
   ],
   "source": [
    "#q_g_dist = np.dot(qf_base, np.transpose(gf)) # TODO: This one will have to be calculated 3 times, and averaged out\n",
    "q_g_dist_base = np.dot(qf_base, np.transpose(gf))\n",
    "q_g_dist_A = np.dot(qf_A, np.transpose(gf))\n",
    "q_g_dist_B = np.dot(qf_B, np.transpose(gf))\n",
    "q_g_dist_C = np.dot(qf_C, np.transpose(gf))\n",
    "q_g_dist = (base_multiplier*q_g_dist_base + 1*q_g_dist_A + 1*q_g_dist_B + 1*q_g_dist_C)/(base_multiplier+number_of_refinements)\n",
    "\n",
    "q_q_dist = np.dot(qf_base, np.transpose(qf_base))\n",
    "g_g_dist = np.dot(gf, np.transpose(gf))\n",
    "\n",
    "logging.info(f'Query_Gallery_dist = {q_g_dist.shape}')\n",
    "logging.info(f'Query_Query_dist = {q_q_dist.shape}')\n",
    "logging.info(f'Galery_Galery_dist = {g_g_dist.shape}')\n",
    "\n",
    "assert q_g_dist.shape[0] == num_query_base\n",
    "assert q_g_dist.shape[1] == num_gallery\n",
    "assert q_q_dist.shape[0] == num_query_base\n",
    "assert q_q_dist.shape[1] == num_query_base\n",
    "assert g_g_dist.shape[0] == num_gallery\n",
    "assert g_g_dist.shape[1] == num_gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033ce1e-1722-4d68-ac70-5a33b9115a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15373332-a53a-4066-8668-653e3c8304a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_rank_dist = re_ranking(q_g_dist, q_q_dist, g_g_dist, k1=re_ranking_k1)\n",
    "indices = np.argsort(re_rank_dist, axis=1)[:, :100]\n",
    "\n",
    "m, n = indices.shape\n",
    "# # logging.info('m: {}  n: {}'.format(m, n))\n",
    "with open(track, 'wb') as f_w:\n",
    "    for i in range(m):\n",
    "        write_line = indices[i] + 1\n",
    "        write_line = ' '.join(map(str, write_line.tolist())) + '\\n'\n",
    "        f_w.write(write_line.encode())\n",
    "\n",
    "\n",
    "lista_nombres = [\"{:06d}.jpg\".format(i) for i in range(1, len(indices) + 1)]\n",
    "output_path = track.split(\".txt\")[0] + \"_submission.csv\"\n",
    "\n",
    "with open(output_path, 'w', newline='') as archivo_csv:\n",
    "    csv_writter = csv.writer(archivo_csv)\n",
    "    csv_writter.writerow(['imageName', 'Corresponding Indexes'])\n",
    "    for numero, track in zip(lista_nombres, indices):\n",
    "        track_str = ' '.join(map(str, track + 1))\n",
    "        csv_writter.writerow([numero, track_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611889a-cc83-4053-976b-9211fbebfc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "320971e8-9d27-4ff3-ad2c-10a813909834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Backuped everything to 1746094978\n"
     ]
    }
   ],
   "source": [
    "#!python update.py --config_file {config_test} --track {os.path.join(model_path, \"track.txt\")}\n",
    "\n",
    "assert os.path.exists(submission_file_name)\n",
    "df = pd.read_csv(submission_file_name)\n",
    "assert df.shape[0] == num_query_base\n",
    "assert df.shape[1] == 2\n",
    "logging.info(f'Backuped everything to {experiment_id}')\n",
    "\n",
    "!cp -r {model_path} {model_path}_backup_{experiment_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12d0c601-aef5-4516-a3d8-67eb92a872ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb6cde00-a380-4511-8500-e74548c374d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Submitting with message \"DSD refinement; commit_hash: 6a355affdb8ee8365325d8dd2aee8df03ea9b715; hyperparameters_train_hash: 8f10df35f3f22a7060923d11debd2f45; hyperparameters_test_hash: 92df54af27bbf9cfde3308cc811d1561; experiment_ID: 1746094978\"\n",
      "100%|██████████| 136k/136k [00:00<00:00, 177kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\"message\": \"Successfully submitted to Urban Elements ReID Challenge\", \"ref\": 44436117}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_message += f\"; commit_hash: {subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('ascii').strip()}\"\n",
    "submission_message += f\"; hyperparameters_train_hash: {calculate_params_hash(hyperparams_train)}\"\n",
    "submission_message += f\"; hyperparameters_test_hash: {calculate_params_hash(hyperparams_test)}\"\n",
    "submission_message += f\"; experiment_ID: {experiment_id}\"\n",
    "logging.info(f'Submitting with message \"{submission_message}\"')\n",
    "\n",
    "# Submit the file to the competition\n",
    "# Uncomment only for actual submissions!\n",
    "api.competition_submit(submission_file_name, submission_message, competition_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3051c2a-6864-455b-8c01-044169bda49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
